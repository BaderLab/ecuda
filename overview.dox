///
/// Justification: a clean C++ interface to CUDA is needed
///
/// The standard library (std::) contains a long established and required set of libaries.
/// Most everyone knows its organization and semantic conventions.
///
/// Problem 1: CUDA device code cannot call std:: because __host__ is assumed
///
/// Example:
/// \code{.cu}
/// template<typename T>
/// __device__ void reverse_sequence( const T* ptr, const std::size_t n ) {
///     std::reverse( ptr, ptr+n );
/// }
/// \endcode
///
/// As natural as this function might be, it will, of course, fail to compile since std::reverse is assumed to be __host__.
///
/// Problem 2: Developer has to pay attention to whether a pointer points to host or device memory
///
/// Example:
/// \code{.cpp}
/// double* startPtr;
/// cudaMalloc( &startPtr, sizeof(double)*1000 );
/// const double* endPtr = startPtr + 1000;
/// std::vector<double> v1( startPtr, endPtr ); // will segfault when any internal attempt to dereference a pointer is made
/// std::vector<double> v2( endPtr-startPtr );
/// std::copy( startPtr, endPtr, v2.begin() ); // will segfault when any internal attempt to dereference a pointer is made
/// cudaFree( startPtr );
/// \endcode
///
/// This will compile fine, but will segfault when run since the host cannot directly dereference a pointer to device memory.
/// The CUDA device driver needs to be utilized. This sucks. We're forced to do something like:
///
/// \code{.cpp}
/// double* startPtr;
/// cudaMalloc( &startPtr, sizeof(double)*1000 );
/// std::vector<double> v( 1000 );
/// cudaMemcpy( &v.front(), startPtr, sizeof(double)*1000, cudaMemcpyDeviceToHost );
/// cudaFree( startPtr );
/// \endcode
///
/// Keeping a vigilant eye on whether a pointer is device or host memory above the additional risks of tossing around
/// raw pointers too much is dangerous.  Ideally, this should be minimized in modern C++ anyway.
///
/// Problem 3: Developer has to pay attention to memory alignment on the device
///
/// Example:
/// \code{.cpp}
/// double* startPtr;
/// size_t pitch;
/// cudaMallocPitch( &startPtr, &pitch, 200, 1000 );
/// \endcode
///
/// Here, we allocate a 200x1000=200,000 sized block of elements.  Typically, the dimensions correspond in some way to
/// CUDA threads (i.e. there are 200 simultaneous threads, each responsible for performing some operation on its own
/// set of 1000 elements).  However, the allocation is not completely contiguous as cudaMallocPitch will hardware align
/// the start of each block of 200 elements, so there may be some padding between each block to achieve this.  Since this
/// strategy is vital for reducing the number of memory read/write operations that must be performed by a pool of threads,
/// we're normally forced to keep track of the "pitch" (the actual width of each row in bytes vs the 200 we explicitly
/// desire).
///
/// Problem 4: the multi-threaded nature of CUDA make container-type data structures more natural to represent as matrices
///
///
///
