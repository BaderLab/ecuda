<!-- HTML header for doxygen 1.8.6-->
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.6"/>
<title>Extended CUDA Library (ecuda): Main Page</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/javascript">
  $(document).ready(function() { searchBox.OnSelectItem(0); });
</script>
<link href="customdoxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td style="padding-left: 0.5em;">
   <div id="projectname">Extended CUDA Library (ecuda)
   &#160;<span id="projectnumber">1.0</span>
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.6 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
  <div id="navrow1" class="tabs">
    <ul class="tablist">
      <li class="current"><a href="index.html"><span>Main&#160;Page</span></a></li>
      <li><a href="pages.html"><span>Related&#160;Pages</span></a></li>
      <li><a href="namespaces.html"><span>Namespaces</span></a></li>
      <li><a href="annotated.html"><span>Classes</span></a></li>
      <li><a href="files.html"><span>Files</span></a></li>
      <li>
        <div id="MSearchBox" class="MSearchBoxInactive">
        <span class="left">
          <img id="MSearchSelect" src="search/mag_sel.png"
               onmouseover="return searchBox.OnSearchSelectShow()"
               onmouseout="return searchBox.OnSearchSelectHide()"
               alt=""/>
          <input type="text" id="MSearchField" value="Search" accesskey="S"
               onfocus="searchBox.OnSearchFieldFocus(true)" 
               onblur="searchBox.OnSearchFieldFocus(false)" 
               onkeyup="searchBox.OnSearchFieldChange(event)"/>
          </span><span class="right">
            <a id="MSearchClose" href="javascript:searchBox.CloseResultsWindow()"><img id="MSearchCloseImg" border="0" src="search/close.png" alt=""/></a>
          </span>
        </div>
      </li>
    </ul>
  </div>
</div><!-- top -->
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
<a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(0)"><span class="SelectionMark">&#160;</span>All</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(1)"><span class="SelectionMark">&#160;</span>Classes</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(2)"><span class="SelectionMark">&#160;</span>Namespaces</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(3)"><span class="SelectionMark">&#160;</span>Files</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(4)"><span class="SelectionMark">&#160;</span>Functions</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(5)"><span class="SelectionMark">&#160;</span>Variables</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(6)"><span class="SelectionMark">&#160;</span>Typedefs</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(7)"><span class="SelectionMark">&#160;</span>Friends</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(8)"><span class="SelectionMark">&#160;</span>Pages</a></div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="headertitle">
<div class="title">Extended CUDA Library (ecuda) Documentation</div>  </div>
</div><!--header-->
<div class="contents">
<div class="toc"><h3>Table of Contents</h3>
<ul><li class="level1"><a href="#intro">Introduction</a><ul><li class="level2"><a href="#overview_thrust">Comparison to the Thrust Library</a></li>
</ul>
</li>
<li class="level1"><a href="#requirements">Requirements</a></li>
<li class="level1"><a href="#installation">Installation</a></li>
<li class="level1"><a href="#overview">Overview</a><ul><li class="level2"><a href="#overview_containers">Containers</a></li>
<li class="level2"><a href="#overview_allocators">Allocators</a></li>
<li class="level2"><a href="#overview_iterators">Iterators</a></li>
<li class="level2"><a href="#overview_views">Views</a></li>
<li class="level2"><a href="#overview_pointers">Specialized Pointers</a></li>
<li class="level2"><a href="#overview_misc">Miscellaneous</a><ul><li class="level3"><a href="#overview_misc_cudawrapper">Capturing CUDA API Errors</a></li>
<li class="level3"><a href="#overview_misc_cstyle">Using C-style Arrays</a></li>
<li class="level3"><a href="#overview_misc_events">CUDA Events</a></li>
</ul>
</li>
<li class="level2"><a href="#overview_misc_stl_algo">STL Functions</a></li>
</ul>
</li>
<li class="level1"><a href="#optimizing_threads">Optimizing Thread Operations</a></li>
<li class="level1"><a href="#performance">Performance</a></li>
<li class="level1"><a href="#compatibility">Compatibility</a><ul><li class="level2"><a href="#overview_cpp11">C++11 Support and CUDA >= 7.0</a></li>
</ul>
</li>
<li class="level1"><a href="#section_examples">Example Programs</a></li>
<li class="level1"><a href="#section_future_work">Future Work</a></li>
<li class="level1"><a href="#license">License</a></li>
<li class="level1"><a href="#author">Author</a></li>
<li class="level1"><a href="#acknowledgements">Acknowledgements</a></li>
</ul>
</div>
<div class="textblock"><h1><a class="anchor" id="intro"></a>
Introduction</h1>
<p>The <em>ecuda</em> library is a set of templates fashioned after the C++ Standard Template Library (STL) that provide several useful containers for use with the CUDA API. These containers remove most of the repetitive low-level tasks that are required when assigning, traversing, and manipulating data in device memory. The containers can be instantiated in host code and passed to kernel functions, so familiar STL-style semantics can be used inside the device code itself. The library is header only, so it's portable and simple to install.</p>
<p>The original motivation for creating the library was to remove a lot of the headaches and inherent lack of safety when using naked pointers to device memory and to create containers that were more intuitive to work with.</p>
<p>For example, here's some typical looking code using the CUDA API that a) creates a matrix and b) uses the GPU to calculate the column sums:</p>
<p><b>Before</b> </p>
<div class="fragment"><div class="line"><span class="keyword">const</span> <span class="keywordtype">size_t</span> width = 100;</div>
<div class="line"><span class="keyword">const</span> <span class="keywordtype">size_t</span> height = 100;</div>
<div class="line"></div>
<div class="line"><span class="comment">// create a 2D array in device memory</span></div>
<div class="line"><span class="keywordtype">double</span>* deviceMatrix;</div>
<div class="line"><span class="keywordtype">size_t</span> pitch;</div>
<div class="line">cudaMalloc2D( &amp;deviceMatrix, &amp;pitch, width*<span class="keyword">sizeof</span>(<span class="keywordtype">double</span>), height );</div>
<div class="line"></div>
<div class="line"><span class="comment">// create linearized row-major matrix in host memory</span></div>
<div class="line">std::vector&lt;double&gt; hostMatrix( width*height );</div>
<div class="line"></div>
<div class="line"><span class="comment">// ... do stuff to prepare matrix data</span></div>
<div class="line"></div>
<div class="line"><span class="comment">// copy matrix from host to device</span></div>
<div class="line"><a class="code" href="namespaceecuda.html#aa6443f3fc662e4015397a140d906662a">cudaMemcpy2D</a>( deviceMatrix, pitch, &amp;hostMatrix.front(), width*<span class="keyword">sizeof</span>(double), width*<span class="keyword">sizeof</span>(<span class="keywordtype">double</span>), height, cudaMemcpyHostToDevice );</div>
<div class="line"></div>
<div class="line"><span class="comment">// create an array on device to hold results</span></div>
<div class="line"><span class="keywordtype">double</span>* deviceColumnSums;</div>
<div class="line">cudaMalloc( &amp;deviceColumnSums, width*<span class="keyword">sizeof</span>(<span class="keywordtype">double</span>) );</div>
<div class="line"><a class="code" href="namespaceecuda.html#a181bf4510a96777fcb4e32a35f309da3">cudaMemset</a>( deviceColumnSums, 0, width*<span class="keyword">sizeof</span>(<span class="keywordtype">double</span>) ); <span class="comment">// set initial values to zero</span></div>
<div class="line"></div>
<div class="line"><span class="comment">// run kernel</span></div>
<div class="line">sumColumns&lt;&lt;&lt;1,width&gt;&gt;&gt;( deviceMatrix, pitch, width, height, deviceColumnSums );</div>
<div class="line">cudaDeviceSynchronize();</div>
<div class="line"></div>
<div class="line"><span class="comment">// create an array with page-locked memory to store results off device</span></div>
<div class="line"><span class="keywordtype">double</span>* hostColumnSums;</div>
<div class="line">cudaMallocHost( &amp;hostColumnSums, width );</div>
<div class="line"></div>
<div class="line"><span class="comment">// copy results from device to host</span></div>
<div class="line"><a class="code" href="namespaceecuda.html#a759c78a1e4125b3f4f3a60f13770c9df">cudaMemcpy</a>( hostColumnSums, deviceColumnSums, width*<span class="keyword">sizeof</span>(<span class="keywordtype">double</span>), cudaMemcpyDeviceToHost );</div>
<div class="line"></div>
<div class="line"><span class="comment">//... do stuff with the results</span></div>
<div class="line"></div>
<div class="line"><span class="comment">// deallocate memory</span></div>
<div class="line">cudaFreeHost( hostColumnSums );</div>
<div class="line">cudaFree( columnSums );</div>
<div class="line">cudaFree( deviceMatrix );</div>
<div class="line"></div>
<div class="line"><span class="comment">// kernel function</span></div>
<div class="line">__global__ <span class="keywordtype">void</span> sumColumns( <span class="keyword">const</span> <span class="keywordtype">char</span>* matrix, <span class="keyword">const</span> <span class="keywordtype">size_t</span> pitch, <span class="keyword">const</span> <span class="keywordtype">size_t</span> width, <span class="keyword">const</span> <span class="keywordtype">size_t</span> height, <span class="keywordtype">double</span>* columnSums ) {</div>
<div class="line"></div>
<div class="line">  <span class="keyword">const</span> <span class="keywordtype">int</span> threadNum = threadIdx.x;</div>
<div class="line">  <span class="keywordflow">if</span>( threadNum &lt; width ) {</div>
<div class="line">    <span class="keywordtype">double</span> columnSum = 0;</div>
<div class="line">    <span class="keyword">const</span> <span class="keywordtype">char</span>* pRow = matrix;</div>
<div class="line">    <span class="keywordflow">for</span>( <span class="keywordtype">size_t</span> i = 0; i &lt; height; ++i ) {</div>
<div class="line">      pRow += pitch; <span class="comment">// move pointer to next row</span></div>
<div class="line">      <span class="keyword">const</span> <span class="keywordtype">double</span>* pElement = <span class="keyword">reinterpret_cast&lt;</span><span class="keyword">const </span><span class="keywordtype">double</span>*<span class="keyword">&gt;</span>(ptr); <span class="comment">// cast to proper type!</span></div>
<div class="line">      columnSum += pElement[threadNum];</div>
<div class="line">    }</div>
<div class="line">    columnSums[threadNum] = columnSum;</div>
<div class="line">  }</div>
<div class="line"></div>
<div class="line">}</div>
</div><!-- fragment --><p>There are so many places to innocently go wrong. Did you remember to specify <code>width*sizeof(double)</code> instead of just <code>width</code>? Did you remember to free the allocated memory? Did you remember to account for the padding in the matrix memory that properly aligns the memory? Did you remember that the padding is in bytes and might not be a strict multiple of the size of the data type you're storing? Are you an adherent to the RAII programming idiom and using CUDA makes you generally uncomfortable?</p>
<p>Here's the equivalent code using <em>ecuda:</em> </p>
<p><b>After</b> </p>
<div class="fragment"><div class="line"><span class="keyword">const</span> <span class="keywordtype">size_t</span> width = 100;</div>
<div class="line"><span class="keyword">const</span> <span class="keywordtype">size_t</span> height = 100;</div>
<div class="line"></div>
<div class="line"><span class="comment">// create 2D matrix on device</span></div>
<div class="line"><a class="code" href="classecuda_1_1matrix.html">ecuda::matrix&lt;double&gt;</a> deviceMatrix( width, height );</div>
<div class="line"></div>
<div class="line"><span class="comment">// create linearized row-major matrix in host memory</span></div>
<div class="line">std::vector&lt;double&gt; hostMatrix( width*height );</div>
<div class="line"></div>
<div class="line"><span class="comment">// ... do stuff to prepare matrix data</span></div>
<div class="line"></div>
<div class="line"><span class="comment">// copy matrix from host to device</span></div>
<div class="line">deviceMatrix.assign( hostMatrix.begin(), hostMatrix.end() );</div>
<div class="line"></div>
<div class="line"><span class="comment">// create vector on device to hold results</span></div>
<div class="line"><a class="code" href="classecuda_1_1vector.html">ecuda::vector&lt;double&gt;</a> deviceColumnSums( width ); </div>
<div class="line"></div>
<div class="line"><span class="comment">// run kernel</span></div>
<div class="line">sumColumns&lt;&lt;&lt;1,width&gt;&gt;&gt;( deviceMatrix, deviceColumnSums );</div>
<div class="line">cudaDeviceSynchronize();</div>
<div class="line"></div>
<div class="line"><span class="comment">// create a vector with page-locked memory to store results off device</span></div>
<div class="line">std::vector&lt; double, ecuda::host_allocator&lt;double&gt; &gt; hostColumnSums( width );</div>
<div class="line"></div>
<div class="line"><span class="comment">// copy results from device to host</span></div>
<div class="line">deviceColumnSums &gt;&gt; hostColumnSums;</div>
<div class="line"></div>
<div class="line"><span class="comment">// ... do stuff with the results</span></div>
<div class="line"></div>
<div class="line"><span class="comment">// kernel function</span></div>
<div class="line">__global__ <span class="keywordtype">void</span> sumColumns( <span class="keyword">const</span> <a class="code" href="classecuda_1_1matrix.html">ecuda::matrix&lt;double&gt;</a> matrix, <a class="code" href="classecuda_1_1vector.html">ecuda::vector&lt;double&gt;</a> columnSums ) {</div>
<div class="line"></div>
<div class="line">  <span class="keyword">const</span> <span class="keywordtype">int</span> threadNum = threadIdx.x;</div>
<div class="line">  <span class="keywordflow">if</span>( threadNum &lt; matrix.<a class="code" href="classecuda_1_1matrix.html#a54d63816227c0ab01aed53486fd9932f">number_columns</a>() ) {</div>
<div class="line">    <span class="keywordtype">double</span> columnSum = 0;</div>
<div class="line">    <a class="code" href="classecuda_1_1sequence__view.html">ecuda::matrix&lt;double&gt;::const_column_type</a> column = matrix.<a class="code" href="classecuda_1_1matrix.html#a673e78abb3a630b5b98cb1a58906c9a3">get_column</a>( threadNum );</div>
<div class="line">    <span class="keywordflow">for</span>( <a class="code" href="classecuda_1_1device__iterator.html">ecuda::matrix&lt;double&gt;::const_column_type::iterator</a> iter = column.<a class="code" href="classecuda_1_1sequence__view.html#a033e5d9295ade2a2854056f488998687">begin</a>(); iter != column.<a class="code" href="classecuda_1_1sequence__view.html#a2f852376a47222ecd785e4b8c3cee613">end</a>(); ++iter )</div>
<div class="line">      columnSum += *iter;</div>
<div class="line">    columnSums[threadNum] = columnSum;</div>
<div class="line">  }</div>
<div class="line"></div>
<div class="line">}</div>
</div><!-- fragment --><p>Besides being more compact and readable, there are no naked pointers, allocation or deallocation operations, or worries about device memory padding. STL semantics like the use of iterators in the kernel function are also much more recognizable. With the very recent addition of C++11 support to the nvcc compiler (yah!), developers using CUDA 7 and later could even replace the column sum loop with:</p>
<div class="fragment"><div class="line"><span class="keywordflow">for</span>( <span class="keywordtype">double</span> x : matrix.<a class="code" href="classecuda_1_1matrix.html#a673e78abb3a630b5b98cb1a58906c9a3">get_column</a>( threadNum ) ) columnSum += x;</div>
</div><!-- fragment --><h2><a class="anchor" id="overview_thrust"></a>
Comparison to the Thrust Library</h2>
<p>Whereas <a href="http://docs.nvidia.com/cuda/thrust/">Thrust</a>, available from NVIDIA, is a CUDA-parallelized <em>replacement</em> of the STL, <em>ecuda</em> is a CUDA-capable <em>extension</em> to the STL. For example, Thrust has parallelized versions of common STL algorithms like sort. It currently has only two containers: thrust::host_vector and thrust:device_vector, whereas <em>ecuda</em> simply has <a class="el" href="classecuda_1_1vector.html" title="A resizable vector stored in device memory. ">ecuda::vector</a> that can work in concert with the existing std::vector. <em>ecuda</em> also focuses on the inclusion of higher-dimensional containers like <a class="el" href="classecuda_1_1matrix.html" title="A resizable matrix stored in device memory. ">ecuda::matrix</a> and <a class="el" href="classecuda_1_1cube.html" title="A resizable cube stored in device memory. ">ecuda::cube</a> which are often an intuitive way to represent data for device-bound tasks. Thrust is also not designed such that you'd pass the containers themselves to kernel functions, whereas this is a central feature of <em>ecuda</em>.</p>
<p>I have had no extensive hands-on experience using Thrust, but I imagine it and <em>ecuda</em> are excellent complementary toolkits. For example:</p>
<p><b>Good</b> task for Thrust:</p>
<div class="fragment"><div class="line"><span class="comment">// I have 1000 measurements from 1000 experiments, and I want to sort each experiment</span></div>
<div class="line"><span class="keywordflow">for</span>( std::size_t i = 0; i &lt; 1000; ++i ) {</div>
<div class="line">  std::vector&lt;double&gt; measurements( 1000 );</div>
<div class="line">  <span class="comment">// ... load measurements for i-th experiment</span></div>
<div class="line">  thrust::sort( measurements.begin(), measurements.end() ); <span class="comment">// parallelized!</span></div>
<div class="line">}</div>
</div><!-- fragment --><p><b>Good</b> task for ecuda:</p>
<div class="fragment"><div class="line"><span class="comment">// I have 1000 measurements from 1000 experiments, and I want to run my fancy stats on each experiment</span></div>
<div class="line"><a class="code" href="classecuda_1_1matrix.html">ecuda::matrix&lt;double&gt;</a> data( 1000, 1000 );</div>
<div class="line"><a class="code" href="classecuda_1_1vector.html">ecuda::vector&lt;double&gt;</a> result( 1000 );</div>
<div class="line"><span class="comment">// ... load measurements</span></div>
<div class="line">runStatistics&lt;&lt;&lt;1,1000&gt;&gt;&gt;( data, result );</div>
<div class="line">CUDA_CALL( cudaDeviceSynchronize() );</div>
<div class="line"></div>
<div class="line">__global__void runStatistics( <span class="keyword">const</span> <a class="code" href="classecuda_1_1matrix.html">ecuda::matrix&lt;double&gt;</a> data, <a class="code" href="classecuda_1_1vector.html">ecuda::vector&lt;double&gt;</a> result ) {</div>
<div class="line">  <span class="keyword">const</span> <span class="keywordtype">int</span> threadNum = threadIdx.x;</div>
<div class="line">  <span class="keywordflow">if</span>( threadNum &lt; data.<a class="code" href="classecuda_1_1matrix.html#a54d63816227c0ab01aed53486fd9932f">number_columns</a>() ) {</div>
<div class="line">    <span class="keywordtype">double</span> fancyStat;</div>
<div class="line">    <a class="code" href="classecuda_1_1sequence__view.html">ecuda::matrix&lt;double&gt;::const_column_type</a> measurements = data.<a class="code" href="classecuda_1_1matrix.html#a673e78abb3a630b5b98cb1a58906c9a3">get_column</a>(threadNum);</div>
<div class="line">    <span class="comment">// ... calculate fancy stat</span></div>
<div class="line">    result[threadNum] = fancyStat;</div>
<div class="line">  }</div>
<div class="line">}</div>
</div><!-- fragment --><h1><a class="anchor" id="requirements"></a>
Requirements</h1>
<p>There are no additional requirements other than your existing toolchain to compile CUDA code (minimally the CUDA API and a C++ compiler).</p>
<h1><a class="anchor" id="installation"></a>
Installation</h1>
<p>The library is header only, so the <code>include</code> subdirectory can be placed anywhere and made visible to the compiler (e.g. <code>-I/some/path/include</code>). A <code>Makefile</code> is included that simply copies the <code>include/ecuda</code> directory to <code>/usr/local/include</code> by issuing the command:</p>
<div class="fragment"><div class="line">sudo make install</div>
</div><!-- fragment --><p>The required containers can be declared in your CUDA code with, for example:</p>
<div class="fragment"><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="array_8hpp.html">ecuda/array.hpp</a>&gt;</span></div>
<div class="line"><span class="preprocessor">#include &lt;<a class="code" href="vector_8hpp.html">ecuda/vector.hpp</a>&gt;</span></div>
<div class="line"><span class="preprocessor">#include &lt;<a class="code" href="matrix_8hpp.html">ecuda/matrix.hpp</a>&gt;</span></div>
<div class="line"><span class="preprocessor">#include &lt;<a class="code" href="cube_8hpp.html">ecuda/cube.hpp</a>&gt;</span></div>
</div><!-- fragment --><p>If you use the CUDA device information or event wrapper then:</p>
<div class="fragment"><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="device_8hpp.html">ecuda/device.hpp</a>&gt;</span></div>
<div class="line"><span class="preprocessor">#include &lt;<a class="code" href="event_8hpp.html">ecuda/event.hpp</a>&gt;</span></div>
</div><!-- fragment --><p>Other headers are utilized internally as needed.</p>
<p>If you have <a href="http://www.doxygen.org">Doxygen</a> installed, you can create a local copy of this documentation with:</p>
<div class="fragment"><div class="line">make docs</div>
</div><!-- fragment --><h1><a class="anchor" id="overview"></a>
Overview</h1>
<h2><a class="anchor" id="overview_containers"></a>
Containers</h2>
<div class="image">
<img src="containers.png" alt="containers.png"/>
<div class="caption">
The four core containers.</div></div>
<p> The library features four containers:</p>
<ol type="1">
<li>fixed-sized <b>array</b> </li>
<li>variable-sized <b>vector</b> </li>
<li>2D <b>matrix</b> </li>
<li>3D <b>cube</b> </li>
</ol>
<p>The <a class="el" href="classecuda_1_1array.html">ecuda::array</a> and <a class="el" href="classecuda_1_1vector.html">ecuda::vector</a> are functionally equivalent to the identically named containers in the STL (with the array being introduced in C++11):</p>
<div class="fragment"><div class="line"><a class="code" href="classecuda_1_1array.html">ecuda::array&lt;double,100&gt;</a> deviceArray; <span class="comment">// create fixed 100 element array</span></div>
<div class="line">deviceArray.<a class="code" href="classecuda_1_1array.html#a8ce14930b529dd0639d388b0317cee6b">fill</a>( 66 ); <span class="comment">// fill array with value 66</span></div>
<div class="line"></div>
<div class="line"><a class="code" href="classecuda_1_1vector.html">ecuda::vector&lt;double&gt;</a> deviceVector1( 100, 66 ); <span class="comment">// create 100 element vector filled with value 66</span></div>
<div class="line">deviceVector1.resize( 200, 99 ); <span class="comment">// expand the vector to 200 elements filling new elements with value 99</span></div>
<div class="line"></div>
<div class="line">std::vector&lt;int&gt; hostVector( 50 ); <span class="comment">// create 50 element vector in host memory</span></div>
<div class="line"><span class="keywordflow">for</span>( <span class="keywordtype">unsigned</span> i = 0; i &lt; 50; ++i ) hostVector[i] = i; <span class="comment">// set the values to range from 0-49</span></div>
<div class="line"><a class="code" href="classecuda_1_1vector.html">ecuda::vector&lt;int&gt;</a> deviceVector2( hostVector.begin(), hostVector.end() ); <span class="comment">// create a vector in device memory and initialize with the host values</span></div>
</div><!-- fragment --><p>The <a class="el" href="classecuda_1_1matrix.html">ecuda::matrix</a> matrix and <a class="el" href="classecuda_1_1cube.html">ecuda::cube</a> cube try to implement STL conventions as faithfully as possible so using them is intuitive.</p>
<div class="fragment"><div class="line"><a class="code" href="classecuda_1_1matrix.html">ecuda::matrix&lt;double&gt;</a> deviceMatrix( 100, 100 ); <span class="comment">// create 100x100 matrix</span></div>
<div class="line"></div>
<div class="line">std::vector&lt;double&gt; hostVector( 100 ); <span class="comment">// create 100 element vector in host memory</span></div>
<div class="line"><span class="keywordflow">for</span>( <span class="keywordtype">unsigned</span> i = 0; i &lt; 100; ++i ) hostVector[i] = i; <span class="comment">// set the values to range from 0-99</span></div>
<div class="line"></div>
<div class="line"><span class="keywordflow">for</span>( <span class="keywordtype">unsigned</span> i = 0; i &lt; 100; ++i ) </div>
<div class="line">   deviceMatrix[i].assign( hostVector.begin(), hostVector.end() ); <span class="comment">// set each row of matrix to hold this sequence of values</span></div>
</div><!-- fragment --><p>The containers can then be passed to kernel functions and manipulated as needed.</p>
<div class="fragment"><div class="line">__global__ </div>
<div class="line"><span class="keywordtype">void</span> addColumns( <span class="keyword">const</span> <a class="code" href="classecuda_1_1matrix.html">ecuda::matrix&lt;double&gt;</a> srcMatrix, <a class="code" href="classecuda_1_1array.html">ecuda::array&lt;double,100&gt;</a> columnSums ) {</div>
<div class="line"></div>
<div class="line">   <span class="keyword">const</span> <span class="keywordtype">int</span> threadNum = threadIdx.x;</div>
<div class="line">   <span class="keyword">typedef</span> <a class="code" href="classecuda_1_1sequence__view.html">ecuda::matrix&lt;double&gt;::const_column_type</a> ColumnType;</div>
<div class="line"></div>
<div class="line">   <span class="keywordflow">if</span>( threadNum &lt; 100 ) {</div>
<div class="line">      ColumnType column = srcMatrix.<a class="code" href="classecuda_1_1matrix.html#a673e78abb3a630b5b98cb1a58906c9a3">get_column</a>(threadNum);</div>
<div class="line">      <span class="keywordflow">for</span>( ColumnType::iterator iter = column.begin(); iter != column.end(); ++iter )</div>
<div class="line">         columnSums[threadNum] += *iter;</div>
<div class="line">   }</div>
<div class="line"></div>
<div class="line">}</div>
<div class="line"></div>
<div class="line"><a class="code" href="classecuda_1_1array.html">ecuda::array&lt;double,100&gt;</a> deviceArray;</div>
<div class="line">addColumns&lt;&lt;&lt;1,100&gt;&gt;&gt;( deviceMatrix, deviceArray );</div>
</div><!-- fragment --><h2><a class="anchor" id="overview_allocators"></a>
Allocators</h2>
<div class="image">
<img src="allocators.png" alt="allocators.png"/>
<div class="caption">
The three memory allocators.</div></div>
<p> STL containers often include an optional "Allocator" template parameter so that memory allocation can be specialized. <em>ecuda</em> uses the same design pattern to handle any allocations of device memory. The default allocator parameter for <em>ecuda</em> containers work fine, so this aspect of the library doesn't require much mention. However, the <a class="el" href="classecuda_1_1host__allocator.html">ecuda::host_allocator</a> allocates page-locked memory and can be useful if you want to use STL containers as a staging point to exchange data between the host and device memory (see the CUDA API documentation for the <code>cudaMallocHost</code> function for a more complete discussion of the advantages and considerations of using page-locked memory).</p>
<p>For example:</p>
<div class="fragment"><div class="line"><a class="code" href="classecuda_1_1vector.html">ecuda::vector&lt;double&gt;</a> deviceVector( 1000 );</div>
<div class="line"><span class="comment">// ... do stuff</span></div>
<div class="line"></div>
<div class="line">std::vector&lt; double, ecuda::host_allocator&lt;double&gt; &gt; hostVector1( 1000 ); <span class="comment">// underlying memory allocated using cudaMallocHost</span></div>
<div class="line">std::vector&lt;double&gt; hostVector2( 1000 ); <span class="comment">// underlying memory allocated using standard &quot;new&quot;</span></div>
<div class="line"></div>
<div class="line">deviceVector &gt;&gt; hostVector1; <span class="comment">// faster</span></div>
<div class="line">deviceVector &gt;&gt; hostVector2; <span class="comment">// slower</span></div>
</div><!-- fragment --><h2><a class="anchor" id="overview_iterators"></a>
Iterators</h2>
<div class="image">
<img src="iterators.png" alt="iterators.png"/>
</div>
<p> Iterators are used extensively in the STL to traverse ranges of elements in a container. They do this more optimally than, say, repeatedly calling the [] operator using an index value. The STL classifies iterators into different "categories" depending on their functionality (from least to most capabilties: Input, Output &lt; Forward &lt; Bidirectional &lt; Random Access). In <em>ecuda</em>, two additional categories are defined: a device_iterator and a contiguous_device_iterator. A reverse iterator, which simply reverses the order of traversal is also available. By design, regular STL iterators are compatible with <em>ecuda</em> containers and views, but not vice versa (since STL containers cannot access device memory).</p>
<p>Under normal use, a developer doesn't have to worry about the iterator classes themselves. They are typically obtained from containers using the <code>begin()</code>, <code>end()</code>, <code>rbegin()</code>, and <code>rend()</code> methods and their type defined as a container-specific <code>typedef</code>. For example:</p>
<div class="fragment"><div class="line"><a class="code" href="classecuda_1_1vector.html">ecuda::vector&lt;int&gt;</a> deviceVector1( 1000 ); <span class="comment">// create 1000 element vector in device memory</span></div>
<div class="line"><a class="code" href="classecuda_1_1contiguous__device__iterator.html">ecuda::vector&lt;int&gt;::iterator</a> begin = deviceVector1.begin();</div>
<div class="line"><a class="code" href="classecuda_1_1vector.html">ecuda::vector&lt;int&gt;</a> deviceVector2( begin, begin+50 ); <span class="comment">// create a new vector initialized with the first 50 elements</span></div>
</div><!-- fragment --><p><em>ecuda</em> containers will also accept standard STL host iterators for most operations.</p>
<div class="fragment"><div class="line"><a class="code" href="classecuda_1_1vector.html">ecuda::vector&lt;int&gt;</a> deviceVector; <span class="comment">// create empty vector in device memory</span></div>
<div class="line">std::vector&lt;int&gt; hostVector( 1000 ); <span class="comment">// create 1000 element vector in host memory</span></div>
<div class="line">deviceVector.<a class="code" href="classecuda_1_1vector.html#a11024cc69b967283013d084390a88b62">assign</a>( hostVector.begin(), hostVector.end() ); <span class="comment">// copy elements from host to device, vector will grow to accomodate</span></div>
</div><!-- fragment --><p>Keep in mind that iterator-based operations that occur on the host that involve device memory requires that the device memory be contiguous (since the code behind the scenes utilizes <code>cudaMemcpy</code> or <code>cudaMemcpy2D</code>). However, code that doesn't follow this rule will simply fail to compile so you'll know that you've violated this.</p>
<div class="fragment"><div class="line"><a class="code" href="classecuda_1_1matrix.html">ecuda::matrix&lt;int&gt;</a> deviceMatrix( 100, 100 );</div>
<div class="line"><a class="code" href="classecuda_1_1contiguous__sequence__view.html">ecuda::matrix&lt;int&gt;::row_type</a> row = deviceMatrix.get_row(0); <span class="comment">// rows are contiguous</span></div>
<div class="line"><a class="code" href="classecuda_1_1sequence__view.html">ecuda::matrix&lt;int&gt;::column_type</a> column = deviceMatrix.get_column(0); <span class="comment">// columns are non-contiguous</span></div>
<div class="line"><a class="code" href="classecuda_1_1vector.html">ecuda::vector&lt;int&gt;</a> deviceVector;</div>
<div class="line">deviceArray.assign( row.<a class="code" href="classecuda_1_1contiguous__sequence__view.html#a14816c832ff68e94515a575785fdf85e">begin</a>(), row.<a class="code" href="classecuda_1_1contiguous__sequence__view.html#af5719deabcfa6f34a496cb4bbf6e0c46">end</a>() ); <span class="comment">// works fine</span></div>
<div class="line">deviceArray.assign( row.<a class="code" href="classecuda_1_1contiguous__sequence__view.html#a78e4c409786016cab7f19a6562ea9d6e">rbegin</a>(), row.<a class="code" href="classecuda_1_1contiguous__sequence__view.html#a5d8750cba15b1d40663d966402b348e3">rend</a>() ); <span class="comment">// won&#39;t compile, elements in the wrong order</span></div>
<div class="line">deviceArray.assign( column.<a class="code" href="classecuda_1_1sequence__view.html#a033e5d9295ade2a2854056f488998687">begin</a>(), column.<a class="code" href="classecuda_1_1sequence__view.html#a2f852376a47222ecd785e4b8c3cee613">end</a>() ); <span class="comment">// won&#39;t compile, elements not in contiguous memory</span></div>
</div><!-- fragment --><p>The same operations in kernel code will compile and work fine though, since element-by-element copying is used.</p>
<div class="fragment"><div class="line">__global__ <span class="keywordtype">void</span> copyColumn( <span class="keyword">const</span> <a class="code" href="classecuda_1_1matrix.html">ecuda::matrix&lt;int&gt;</a> matrix, <a class="code" href="classecuda_1_1vector.html">ecuda::vector&lt;int&gt;</a> vector ) {</div>
<div class="line">  <a class="code" href="classecuda_1_1sequence__view.html">ecuda::matrix&lt;int&gt;::const_column_type</a> column = matrix.<a class="code" href="classecuda_1_1matrix.html#a673e78abb3a630b5b98cb1a58906c9a3">get_column</a>(0);</div>
<div class="line">  vector.<a class="code" href="classecuda_1_1vector.html#a11024cc69b967283013d084390a88b62">assign</a>( column.<a class="code" href="classecuda_1_1sequence__view.html#a033e5d9295ade2a2854056f488998687">begin</a>(), column.<a class="code" href="classecuda_1_1sequence__view.html#a2f852376a47222ecd785e4b8c3cee613">end</a>() ); <span class="comment">// now compiles and works fine</span></div>
<div class="line">}</div>
</div><!-- fragment --><p>Copying device data to host containers should be done through the &gt;&gt; operator, which is defined for all containers and contiguous views.</p>
<div class="fragment"><div class="line"><a class="code" href="classecuda_1_1vector.html">ecuda::vector&lt;int&gt;</a> deviceVector( 1000 );</div>
<div class="line"><span class="comment">// ... do stuff to device vector</span></div>
<div class="line">std::vector&lt;int&gt; hostVector( 1000 );</div>
<div class="line">deviceVector &gt;&gt; hostVector; <span class="comment">// copy vector contents to host</span></div>
<div class="line"></div>
<div class="line"><a class="code" href="classecuda_1_1matrix.html">ecuda::matrix&lt;int&gt;</a> deviceMatrix( 10, 1000 );</div>
<div class="line"><span class="comment">// ... do stuff to device matrix</span></div>
<div class="line">std::vector&lt;int&gt; hostRow( 1000 );</div>
<div class="line">deviceMatrix[5] &gt;&gt; hostRow; <span class="comment">// copy individial matrix row to host</span></div>
<div class="line">std::vector&lt;int&gt; hostMatrix( 10*1000 );</div>
<div class="line">deviceMatrix &gt;&gt; hostMatrix; <span class="comment">// copy entire matrix to host</span></div>
</div><!-- fragment --><h2><a class="anchor" id="overview_views"></a>
Views</h2>
<div class="image">
<img src="views.png" alt="views.png"/>
</div>
<p> The core <em>ecuda</em> containers can be used without having to directly create a view, but it can be useful to know they exist since they are returned by methods such as <a class="el" href="classecuda_1_1matrix.html" title="A resizable matrix stored in device memory. ">ecuda::matrix</a>'s <code>get_row</code>. These act as views of a subset of an existing container without carrying responsibility for memory [de]allocation. For example, a pointer to the start of a single row or column of a <a class="el" href="classecuda_1_1matrix.html">ecuda::matrix</a> is provided to a <a class="el" href="classecuda_1_1contiguous__sequence__view.html">ecuda::contiguous_sequence_view</a> or <a class="el" href="classecuda_1_1sequence__view.html">ecuda::sequence_view</a>, respectively, so that the memory in the matrix can be traversed and acted upon as a sequence. Similarly, a slice of a <a class="el" href="classecuda_1_1cube.html">ecuda::cube</a> along the x-y, x-z, or y-z plane would be represented as a <a class="el" href="classecuda_1_1matrix__view.html">ecuda::matrix_view</a> or <a class="el" href="classecuda_1_1contiguous__matrix__view.html">ecuda::contiguous_matrix_view</a>. Like iterators, they are aliased through a container-specific <code>typedef</code> and aren't referenced explicity under normal use. For example:</p>
<div class="fragment"><div class="line"><a class="code" href="classecuda_1_1matrix.html">ecuda::matrix&lt;int&gt;</a> deviceMatrix( 100, 100 ); <span class="comment">// create 100x100 matrix</span></div>
<div class="line"><a class="code" href="classecuda_1_1contiguous__sequence__view.html">ecuda::matrix&lt;int&gt;::row_type</a> row = deviceMatrix.get_row(0); <span class="comment">// is a contiguous_sequence_view</span></div>
<div class="line"><a class="code" href="classecuda_1_1sequence__view.html">ecuda::matrix&lt;int&gt;::column_type</a> column = deviceMatrix.get_row(0); <span class="comment">// is a sequence_view</span></div>
<div class="line"></div>
<div class="line"><a class="code" href="classecuda_1_1cube.html">ecuda::cube&lt;int&gt;</a> deviceCube( 100, 100, 100 ); <span class="comment">// create 100x100x100 cube</span></div>
<div class="line"><a class="code" href="classecuda_1_1matrix__view.html">ecuda::cube&lt;int&gt;::slice_xy_type</a> xy = deviceCube.get_xy(0); <span class="comment">// is a matrix_view</span></div>
<div class="line"><a class="code" href="classecuda_1_1contiguous__matrix__view.html">ecuda::cube&lt;int&gt;::slice_xz_type</a> xz = deviceCube.get_xz(0); <span class="comment">// is a contiguous_matrix_view</span></div>
</div><!-- fragment --><h2><a class="anchor" id="overview_pointers"></a>
Specialized Pointers</h2>
<dl class="section warning"><dt>Warning</dt><dd>This section is optional reading since a developer doesn't interact with specialized pointers directly. They are used by the API internally.</dd></dl>
<div class="image">
<img src="pointers.png" alt="pointers.png"/>
</div>
<p> <a class="el" href="classecuda_1_1device__ptr.html">ecuda::device_ptr</a> is a reference-counting smart pointer to device memory which takes responsibility over an allocation of device memory and then automatically deallocates it when the reference count reaches zero. It's quite similar to a std::shared_ptr. All <em>ecuda</em> containers use a <a class="el" href="classecuda_1_1device__ptr.html">ecuda::device_ptr</a> to interface with any underlying device memory.</p>
<p><a class="el" href="classecuda_1_1striding__ptr.html">ecuda::striding_ptr</a> and <a class="el" href="classecuda_1_1padded__ptr.html">ecuda::padded_ptr</a> are pointer-like classes (allow all of the same operations as naked pointers) but abstract away any non-contiguity in the memory being traversed. For example, traversing the elements of a particular column of a row-major matrix is facilitated through the use of a <a class="el" href="classecuda_1_1striding__ptr.html">ecuda::striding_ptr</a> where the "stride" is set to the number of columns. CUDA also allocates 2D memory in a way that optimizes read/write operations across many threads (see <a class="el" href="index.html#optimizing_threads">Optimizing Thread Operations</a> below for more discussion on this), and results in empty padding at the end of each row. A <a class="el" href="classecuda_1_1padded__ptr.html">ecuda::padded_ptr</a> is used to traverse these elements while automatically skipping the padding. These two specialized pointers can also be combined. They're used internally to create efficient views and iterators.</p>
<h2><a class="anchor" id="overview_misc"></a>
Miscellaneous</h2>
<p>The library contains additional classes, functions, and macros to simplify some common CUDA programming tasks.</p>
<h3><a class="anchor" id="overview_misc_cudawrapper"></a>
Capturing CUDA API Errors</h3>
<p>A macro called CUDA_CALL is defined that captures errors from any of the low-level CUDA API functions and throws an <a class="el" href="classecuda_1_1cuda__error.html">ecuda::cuda_error</a> exception.</p>
<div class="fragment"><div class="line"><span class="keywordflow">try</span> {</div>
<div class="line">  CUDA_CALL( cudaDeviceSynchronize() );</div>
<div class="line">} <span class="keywordflow">catch</span>( <a class="code" href="classecuda_1_1cuda__error.html">ecuda::cuda_error</a>&amp; ex ) {</div>
<div class="line">  std::cerr &lt;&lt; <span class="stringliteral">&quot;CUDA API error: &quot;</span> &lt;&lt; ex.what() &lt;&lt; std::endl;</div>
<div class="line">}</div>
</div><!-- fragment --><p>The macro is also used with any such calls within the <em>ecuda</em> library.</p>
<p>Additionally, a macro called CUDA_CHECK_ERRORS can be used after starting a kernel to capture any error that would normally be checked using cudaGetLastError(). Any error will be converted to an <a class="el" href="classecuda_1_1cuda__error.html">ecuda::cuda_error</a> exception and thrown.</p>
<div class="fragment"><div class="line"><span class="keywordflow">try</span> {</div>
<div class="line">  myKernel&lt;&lt;&lt;10,1000&gt;&gt;&gt;( ... );</div>
<div class="line">  CUDA_CHECK_ERRORS();</div>
<div class="line">} <span class="keywordflow">catch</span>( <a class="code" href="classecuda_1_1cuda__error.html">ecuda::cuda_error</a>&amp; ex ) {</div>
<div class="line">  std::cerr &lt;&lt; <span class="stringliteral">&quot;CUDA kernel error: &quot;</span> &lt;&lt; ex.what() &lt;&lt; std::endl;</div>
<div class="line">}</div>
</div><!-- fragment --><h3><a class="anchor" id="overview_misc_cstyle"></a>
Using C-style Arrays</h3>
<p>The <a class="el" href="classecuda_1_1host__array__proxy.html">ecuda::host_array_proxy</a> is a simple wrapper that allows C-style arrays to function as a standard container. This is useful for incorporating other libraries, like the GNU Scientific Library, that still use naked pointers.</p>
<div class="fragment"><div class="line">gsl_matrix* mat = gsl_matrix_alloc( 10, 20 );</div>
<div class="line"><span class="comment">// ... prepare matrix values</span></div>
<div class="line"><a class="code" href="classecuda_1_1host__array__proxy.html">ecuda::host_array_proxy&lt;double&gt;</a> proxy( mat-&gt;data, 10*20 );</div>
<div class="line"><a class="code" href="classecuda_1_1matrix.html">ecuda::matrix&lt;double&gt;</a> deviceMatrix( 10, 20 );</div>
<div class="line">deviceMatrix.assign( proxy.begin(), proxy.end() ); <span class="comment">// copies gsl_matrix to device matrix via the proxy</span></div>
<div class="line">deviceMatrix &gt;&gt; proxy; <span class="comment">// copies device matrix to gsl_matrix via the proxy</span></div>
</div><!-- fragment --><h3><a class="anchor" id="overview_misc_events"></a>
CUDA Events</h3>
<p>A wrapper around CUDA event objects called <a class="el" href="classecuda_1_1event.html">ecuda::event</a> makes these more C++-like.</p>
<div class="fragment"><div class="line"><span class="comment">// record kernel execution time</span></div>
<div class="line"><a class="code" href="classecuda_1_1event.html">ecuda::event</a> start, stop;</div>
<div class="line"></div>
<div class="line">start.<a class="code" href="classecuda_1_1event.html#aea6c8159da636ccf4481460b97c1781f">record</a>(); <span class="comment">// record start time</span></div>
<div class="line">myKernel&lt;&lt;&lt;10,1000&gt;&gt;&gt;( ... );</div>
<div class="line">stop.<a class="code" href="classecuda_1_1event.html#aea6c8159da636ccf4481460b97c1781f">record</a>(); <span class="comment">// record stop time</span></div>
<div class="line"></div>
<div class="line">stop.<a class="code" href="classecuda_1_1event.html#aa3c2ce52eaf99b88550a0dfcb79d077f">synchronize</a>(); <span class="comment">// kernel execution is asynchronous, wait until it finishes</span></div>
<div class="line"></div>
<div class="line">std::cerr &lt;&lt; <span class="stringliteral">&quot;EXECUTION TIME: &quot;</span> &lt;&lt; ( stop-start ) &lt;&lt; <span class="stringliteral">&quot;milliseconds&quot;</span> &lt;&lt; std::endl;</div>
</div><!-- fragment --><h2><a class="anchor" id="overview_misc_stl_algo"></a>
STL Functions</h2>
<p>Some standard STL functions are re-implemented, or extended to work in kernels or utilize <em>ecuda</em> classes. For example, <code>max_element</code>, <code>swap</code>, <code>copy</code>, <code>distance</code> etc. A <a href="namespaceecuda.html">complete list</a> is available in the generated documentation. These were implemented only to support specific tasks within the API, so <em>ecuda</em> does not exhaustively treat the many STL functions in headers like &lt;algorithm&gt;, &lt;iterator&gt;, or &lt;type_traits&gt;.</p>
<div class="fragment"><div class="line">std::vector&lt;int&gt; hostVectorFragment1( 50, 99 );</div>
<div class="line">std::vector&lt;int&gt; hostVectorFragment1( 50, 66 );</div>
<div class="line"><a class="code" href="classecuda_1_1vector.html">ecuda::vector&lt;int&gt;</a> deviceVector( 100 );</div>
<div class="line"><a class="code" href="namespaceecuda.html#a9a80e5f8f27e09d32181797366876fc1">ecuda::copy</a>( hostVectorFragment1.begin(), hostVectorFragment1.end(), deviceVector.<a class="code" href="classecuda_1_1vector.html#a2467abdd66dae3c6a8172f9cdc7568f6">begin</a>() ); <span class="comment">// works with device iterators</span></div>
<div class="line"><a class="code" href="namespaceecuda.html#a9a80e5f8f27e09d32181797366876fc1">ecuda::copy</a>( hostVectorFragment2.begin(), hostVectorFragment2.end(), deviceVector.<a class="code" href="classecuda_1_1vector.html#a2467abdd66dae3c6a8172f9cdc7568f6">begin</a>()+50 ); <span class="comment">// works with device iterators</span></div>
<div class="line"><a class="code" href="namespaceecuda.html#a9a80e5f8f27e09d32181797366876fc1">std::copy</a>( hostVectorFragment1.begin(), hostVectorFragment1.end(), deviceVector.<a class="code" href="classecuda_1_1vector.html#a2467abdd66dae3c6a8172f9cdc7568f6">begin</a>() ); <span class="comment">// compiler barfs, STL doesn&#39;t recognize device iterators</span></div>
</div><!-- fragment --><h1><a class="anchor" id="optimizing_threads"></a>
Optimizing Thread Operations</h1>
<p>One of the central concerns when CUDA programming is that read/write operations to device memory are extremely time-consuming, so organizing these operations so that different threads access data in close physical proximity greatly optimizes the program. This can be a source of optimization with any program, but it is particularly impactful for multi-threading with GPUs. The specifics depend on the hardware, but the bus might always transfer say, 128 bits of memory per read operation. If each data element is 8 bytes then a single read can potentially supply 16 threads with the information it requires. When using the <a class="el" href="classecuda_1_1matrix.html">ecuda::matrix</a> matrix and <a class="el" href="classecuda_1_1cube.html">ecuda::cube</a> containers, it's important to note how these are represented in memory and which dimension different threads should preferably access. The rule of thumb is that the minor dimension should be the target of separate threads. For a matrix, this is a column; for a cube, this is a depth (and secondarily, a column). For example, given these two kernel functions:</p>
<div class="fragment"><div class="line"><span class="comment">// kernel #1</span></div>
<div class="line">__global__ <span class="keywordtype">void</span> rowSums( <span class="keyword">const</span> <a class="code" href="classecuda_1_1matrix.html">ecuda::matrix&lt;double&gt;</a> matrix, <a class="code" href="classecuda_1_1vector.html">ecuda::vector&lt;double&gt;</a> sums ) {</div>
<div class="line">  <span class="keyword">const</span> <span class="keywordtype">int</span> threadNum = threadIdx.x;</div>
<div class="line">  <span class="keywordflow">if</span>( threadNum &lt; matrix.<a class="code" href="classecuda_1_1matrix.html#a00cd78b7556febca832023c4d6ef3812">number_rows</a>() ) {</div>
<div class="line">    <span class="keywordtype">double</span> sum = 0.0;</div>
<div class="line">    <span class="keywordflow">for</span>( <span class="keywordtype">double</span> x : matrix.<a class="code" href="classecuda_1_1matrix.html#a92fec8d606dfb1084a92275c78e5159e">get_row</a>(threadNum) ) sum += x; <span class="comment">// C++11 range-based loop (CUDA &gt;=7.0)</span></div>
<div class="line">    sums[threadNum] = sum;</div>
<div class="line">  }</div>
<div class="line">}</div>
<div class="line"></div>
<div class="line"><span class="comment">// kernel #2</span></div>
<div class="line">__global__ <span class="keywordtype">void</span> columnSums( <span class="keyword">const</span> <a class="code" href="classecuda_1_1matrix.html">ecuda::matrix&lt;double&gt;</a> matrix, <a class="code" href="classecuda_1_1vector.html">ecuda::vector&lt;double&gt;</a> sums ) {</div>
<div class="line">  <span class="keyword">const</span> <span class="keywordtype">int</span> threadNum = threadIdx.x;</div>
<div class="line">  <span class="keywordflow">if</span>( threadNum &lt; matrix.<a class="code" href="classecuda_1_1matrix.html#a54d63816227c0ab01aed53486fd9932f">number_columns</a>() ) {</div>
<div class="line">    <span class="keywordtype">double</span> sum = 0.0;</div>
<div class="line">    <span class="keywordflow">for</span>( <span class="keywordtype">double</span> x : matrix.<a class="code" href="classecuda_1_1matrix.html#a673e78abb3a630b5b98cb1a58906c9a3">get_column</a>(threadNum) ) sum += x; <span class="comment">// C++11 range-based loop (CUDA &gt;=7.0)</span></div>
<div class="line">    sums[threadNum] = sum;</div>
<div class="line">  }</div>
<div class="line">}</div>
<div class="line"></div>
<div class="line"><span class="comment">// host code</span></div>
<div class="line"><a class="code" href="classecuda_1_1matrix.html">ecuda::matrix&lt;double&gt;</a> deviceMatrix( 1000, 1000 ); <span class="comment">// create 1000x1000 matrix</span></div>
<div class="line"><a class="code" href="classecuda_1_1vector.html">ecuda::vector&lt;double&gt;</a> deviceVector( 1000 ); <span class="comment">// create zero-initialized vector</span></div>
<div class="line"><span class="comment">// ... put stuff into deviceMatrix</span></div>
<div class="line">{</div>
<div class="line">  <a class="code" href="classecuda_1_1event.html">ecuda::event</a> start, stop;</div>
<div class="line">  start.<a class="code" href="classecuda_1_1event.html#aea6c8159da636ccf4481460b97c1781f">record</a>();</div>
<div class="line">  rowSums&lt;&lt;&lt;1,1000&gt;&gt;&gt;( deviceMatrix, deviceVector );</div>
<div class="line">  stop.<a class="code" href="classecuda_1_1event.html#aea6c8159da636ccf4481460b97c1781f">record</a>();</div>
<div class="line">  stop.<a class="code" href="classecuda_1_1event.html#aa3c2ce52eaf99b88550a0dfcb79d077f">synchronize</a>();</div>
<div class="line">  std::cout &lt;&lt; <span class="stringliteral">&quot;EXECUTION TIME: &quot;</span> &lt;&lt; (stop-start) &lt;&lt; <span class="stringliteral">&quot;ms&quot;</span> &lt;&lt; std::endl;</div>
<div class="line">}</div>
<div class="line">{</div>
<div class="line">  <a class="code" href="classecuda_1_1event.html">ecuda::event</a> start, stop;</div>
<div class="line">  start.<a class="code" href="classecuda_1_1event.html#aea6c8159da636ccf4481460b97c1781f">record</a>();</div>
<div class="line">  columnSums&lt;&lt;&lt;1,1000&gt;&gt;&gt;( deviceMatrix, deviceVector );</div>
<div class="line">  stop.<a class="code" href="classecuda_1_1event.html#aea6c8159da636ccf4481460b97c1781f">record</a>();</div>
<div class="line">  stop.<a class="code" href="classecuda_1_1event.html#aa3c2ce52eaf99b88550a0dfcb79d077f">synchronize</a>();</div>
<div class="line">  std::cout &lt;&lt; <span class="stringliteral">&quot;EXECUTION TIME: &quot;</span> &lt;&lt; (stop-start) &lt;&lt; <span class="stringliteral">&quot;ms&quot;</span> &lt;&lt; std::endl;</div>
<div class="line">}</div>
</div><!-- fragment --><p>The execution time of the second kernel will invariably be <em>much</em> faster than the first. These considerations are also discussed and expanded on in the generated documentation for each container.</p>
<h1><a class="anchor" id="performance"></a>
Performance</h1>
<p>Considerable effort was dedicated to minimizing the overhead of using <em>ecuda</em>. Several benchmarks suggest that the overhead is trivial, with no consistent difference between programs using direct CUDA calls and those utilizing <em>ecuda</em> containers. If you're not an experienced CUDA developer, there will likely be a performance increase if your normal approach would not have been optimized and there will certainly be less headaches with raw pointers. If you're already familiar with C++/STL <em>ecuda</em> should greatly ease your introduction to CUDA programming. There are several programs in the <code>benchmarks</code> folder you can compile to look at the performance and the design of the test problems used. They are not optimal implementations of a matrix transpose or multiplication by any means, but are solely designed to identify any performance costs from the additional <em>ecuda</em> layer over direct CUDA API calls.</p>
<p>On a test machine containing an NVIDIA Tesla M2090 GPU and Intel Xeon E5-2620 CPU, some results were:</p>
<table class="doxtable">
<tr>
<th>Program </th><th>Matrix Size </th><th>CPU only </th><th>GPU + CUDA </th><th>GPU + ecuda  </th></tr>
<tr>
<td>matrix_tranpose </td><td>10000x10000 </td><td>1420ms </td><td>44ms </td><td>44ms </td></tr>
<tr>
<td>matrix_multiply </td><td>1000x1000 </td><td>9760ms </td><td>40ms </td><td>40ms </td></tr>
<tr>
<td>matrix_multiply </td><td>5000x5000 </td><td>~18min </td><td>4900ms </td><td>4900ms </td></tr>
</table>
<h1><a class="anchor" id="compatibility"></a>
Compatibility</h1>
<p>The library has been tested and compiles successfully with CUDA versions 5.0, 5.5, 6.0, and 7.0 in combination with GCC 4.8.1. CUDA 6.0 and 7.0 with GCC 4.8.2 or Clang 3.5 also compiled successfully but no example programs were tested. CUDA &lt;5.0 is not supported (specifically, CUDA 3.2, 4.0, 4.1, and 4.2 were tested and did not respect the preprocessor directives in __host__/__device__ methods that create a host-specific and device-specific implementation).</p>
<p>The only pernicious bug I encountered was in an experimental version of the API (that was ultimately not adopted for final release), when using test programs with a larger number (&gt;10) of separate kernels in the same file. Specifically, this code fragment:</p>
<div class="fragment"><div class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt;</div>
<div class="line">__global__ <span class="keywordtype">void</span> linearizeMatrix( <span class="keyword">const</span> <a class="code" href="classecuda_1_1matrix.html">ecuda::matrix&lt;T&gt;</a> matrix, <a class="code" href="classecuda_1_1vector.html">ecuda::vector&lt;T&gt;</a> vector ) {</div>
<div class="line">  <span class="comment">// use iterator to get data from src, and index to set data in dest</span></div>
<div class="line">  std::size_t index = 0;</div>
<div class="line">  <span class="keywordflow">for</span>( <span class="keyword">typename</span> <a class="code" href="classecuda_1_1device__iterator.html">ecuda::matrix&lt;T&gt;::const_iterator</a> iter = matrix.<a class="code" href="classecuda_1_1matrix.html#a1a6c98eb22a8c57990818eb045dab534">begin</a>(); iter != matrix.<a class="code" href="classecuda_1_1matrix.html#af795712961534a0667aa8b8b2e8afdd1">end</a>(); ++iter, ++index ) vector[index] = *iter;</div>
<div class="line">}</div>
</div><!-- fragment --><p>The value of <code>index</code> would not increment. The issue disappeared if I ran only that kernel in CUDA 5.5, and was never an issue regardless of how many kernels were run in CUDA 6.0. I concluded that it was a problem with the CUDA 5.5 <code>nvcc</code> compiler, but I'm including it here for transparency and in case a similar issue arises again.</p>
<p><em>ecuda</em> has not been tested on Windows platforms, although it should be straightforward to drop into a Visual Studio project and check. I seem to recall there were issues with using <b>and</b> and <b>or</b> (a personal C++ idiosyncracy of mine) in place of <b><code>&amp;&amp;</code></b> and <b><code>||</code></b> with Windows compilers in other code I've worked on. If this is real problem I'm remembering correctly, I will replace these in a future release. YMMV, but I expect any problems can be easily identified and hacked into working form. Please contact me if you run into issues.</p>
<h2><a class="anchor" id="overview_cpp11"></a>
C++11 Support and CUDA &gt;= 7.0</h2>
<p>C++11 support was finally added to CUDA 7.0. <em>ecuda</em> does implement the additions to the STL specification that came with C++11. These have been included in <em>ecuda</em> for quite some time in an attempt to future-proof the API, and it's paid off just in time for release. The move constructor is defined, std::initializer_list is supported, and the methods cbegin(), cend(), crbegin(), and crend() to explicitly acquire constant iterators even if the container they are being requested from is non-const. The API has not been extensively tested with CUDA 7.0, but test programs that utilize these features compile without issue.</p>
<p>The keywords <code>constexpr</code> and <code>noexcept</code> are also utilized (the __CONSTEXPR__ and __NOEXCEPT__ macros, which can be seen in the generated documentation for various methods, indicate these keywords and they are enabled only if C++11 support is detected), so some minor performance benefit might occur when compiling with CUDA 7.0 (and the -std=c++11 flag).</p>
<h1><a class="anchor" id="section_examples"></a>
Example Programs</h1>
<p>There are a number of programs in the <code>test</code>, <code>examples</code>, and <code>benchmarks</code> subdirectories of the release. These should all compile with GNU Make:</p>
<div class="fragment"><div class="line">make test/test_array</div>
<div class="line">make examples/euclidean</div>
<div class="line">make benchmarks/matrix_transpose</div>
<div class="line">... and so forth</div>
</div><!-- fragment --><p>You may have to change the Makefile parameters to reflect your setup if you're using a compiler other than GCC or have CUDA installed in a non-standard location. Look for:</p>
<div class="fragment"><div class="line">CXX = g++</div>
<div class="line">CXXFLAGS = -O3 -Wall -flto -L/usr/local/cuda/lib64 -pedantic</div>
<div class="line">NVCC = /usr/local/cuda/bin/nvcc</div>
<div class="line">NVCCFLAGS = -arch=sm_21 -O3</div>
<div class="line">LDLIBS = -lcudart</div>
</div><!-- fragment --><p>Alternatively, the Makefile will take any variables defined in a file named <code>local-config.cfg</code> if it is present, so you can simply create that file and redeclare the relevant lines with different values there.</p>
<p>The programs are not intended to be pretty or well organized, but they will give a taste of how to use the API in practice. They're basically hacky code that I used to check that the API was performing tasks correctly.</p>
<h1><a class="anchor" id="section_future_work"></a>
Future Work</h1>
<p>I'll continue to develop this as time permits. The specifications for the core container should remain fixed, so I don't anticipate any backwards compatibility issues from any future improvements.</p>
<p>There are a number of architectural changes that are possible to the API that are on a to-do list. In one ill-fated trip down the C++ template/metaprogramming rabbit hole, I was able to make containers subclasses of views, and reworked specialized pointers in a way that was more appealing. Unfortunately, the API took a huge performance hit. It's well-known that too much templating eventually hinders C++ compilers (and presumably the nvcc compiler) from making good optimizations, so this is something to keep in mind. As of now, the design is in a good place as far as performance versus code duplication.</p>
<p>Finally, apologies in advance for any cosmetic blemishes in variable names and documentation. These will be cleaned up as they are found.</p>
<h1><a class="anchor" id="license"></a>
License</h1>
<p>The <em>ecuda</em> library is open source and released under the FreeBSD license.</p>
<pre class="fragment">Redistribution and use in source and binary forms, with or without
modification, are permitted provided that the following conditions are met:

1. Redistributions of source code must retain the above copyright notice, this
   list of conditions and the following disclaimer.
2. Redistributions in binary form must reproduce the above copyright notice,
   this list of conditions and the following disclaimer in the documentation
   and/or other materials provided with the distribution.

THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND
ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR
ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
(INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.

The views and conclusions contained in the software and documentation are those
of the authors and should not be interpreted as representing official policies,
either expressed or implied, of the FreeBSD Project.
</pre><h1><a class="anchor" id="author"></a>
Author</h1>
<p>Scott Zuyderduyn, Ph.D.<br/>
 Postdoctoral Research Fellow<br/>
 Bader Lab<br/>
 The University of Toronto<br/>
 <br/>
 Email: scott.zuyderduyn *at* utoronto.ca</p>
<h1><a class="anchor" id="acknowledgements"></a>
Acknowledgements</h1>
<p>The resources and expertise of the <a href="http://www.scinethpc.ca">SciNet</a> supercomputing centre at The University of Toronto which is home to several GPU clusters. I used these extensively for my own scientific research (which spawned the creation of this library).</p>
<p>The support of the <a href="http://baderlab.org/">Bader Lab</a>, part of the <a href="http://tdccbr.med.utoronto.ca">Donnelly Centre for Cellular and Biomolecular Research</a> at The University of Toronto, where I am currently a postdoctoral fellow. </p>
</div></div><!-- contents -->
<!-- HTML footer for doxygen 1.8.6-->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated on Mon Apr 13 2015 17:40:41 for Extended CUDA Library (ecuda) by &#160;<a href="http://www.doxygen.org/index.html">
<img class="footer" src="doxygen.png" alt="doxygen"/>
</a> 1.8.6
</small></address>
</body>
</html>
