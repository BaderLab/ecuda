/**
\mainpage

\tableofcontents

\section intro Introduction

The \em ecuda library is a set of templates fashioned after the C++ Standard Template Library (STL) that provide several useful containers for use with the CUDA API.  These containers remove most of the repetitive low-level tasks that are required when assigning, traversing, and manipulating data in device memory.  The containers can be instantiated in host code and passed to kernel functions, so familiar STL-style semantics can be used inside the device code itself.  The library is header only, so it's portable and simple to install.

The original motivation for creating the library was to remove a lot of the headaches and inherent lack of safety when using naked pointers to device memory.  For example, here's some typical looking code using the CUDA API that a) creates a matrix and b) uses the GPU to calculate the column sums:

\code{.cpp}
const size_t width = 100;
const size_t height = 100;

// create a 2D array in device memory
double* deviceMatrix;
size_t pitch;
cudaMalloc2D( &deviceMatrix, &pitch, width*sizeof(double), height );

// create linearized row-major matrix in host memory
std::vector<double> hostMatrix( width*height );

// ... do stuff to prepare matrix data

// copy matrix from host to device
cudaMemcpy2D( deviceMatrix, pitch, &hostMatrix.front(), width*sizeof(double), width*sizeof(double), height, cudaMemcpyHostToDevice );

// create an array on device to hold results
double* deviceColumnSums;
cudaMalloc( &deviceColumnSums, width*sizeof(double) );
cudaMemset( deviceColumnSums, 0, width*sizeof(double) ); // set initial values to zero

// run kernel
sumColumns<<<1,width>>>( deviceMatrix, pitch, width, height, deviceColumnSums );
cudaDeviceSynchronize();

// create an array with page-locked memory to store results off device
double* hostColumnSums;
cudaMallocHost( &hostColumnSums, width );

// copy results from device to host
cudaMemcpy( hostColumnSums, deviceColumnSums, width*sizeof(double), cudaMemcpyDeviceToHost );

//... do stuff with the results

// deallocate memory
cudaFreeHost( hostColumnSums );
cudaFree( columnSums );
cudaFree( deviceMatrix );

// kernel function
__global__ void sumColumns( const char* matrix, const size_t pitch, const size_t width, const size_t height, double* columnSums ) {

  const int threadNum = threadIdx.x;
  if( threadNum < width ) {
    double columnSum = 0;
    const char* pRow = matrix;
    for( size_t i = 0; i < height; ++i ) {
      pRow += pitch; // move pointer to next row
      const double* pElement = reinterpret_cast<const double*>(ptr); // cast to proper type!
      columnSum += pElement[threadNum];
    }
    columnSums[threadNum] = columnSum;
  }

}
\endcode

There are so many places to innocently go wrong.  Did you remember to specify <tt>width*sizeof(double)</tt> instead of just <tt>width</tt>?  Did you remember to free the allocated memory?  Did you remember to account for the padding in the matrix memory that properly aligns the memory?  Did you remember that the padding is in bytes and might not be a strict multiple of the size of the data type you're storing?  Are you an adherent to the RAII programming idiom and using CUDA makes you generally uncomfortable?

Here's the equivalent code using \em ecuda:

\code{.cpp}
const size_t width = 100;
const size_t height = 100;

// create 2D matrix on device
ecuda::matrix<double> deviceMatrix( width, height );

// create linearized row-major matrix in host memory
std::vector<double> hostMatrix( width*height );

// ... do stuff to prepare matrix data

// copy matrix from host to device
deviceMatrix.assign( hostMatrix.begin(), hostMatrix.end() );

// create vector on device to hold results
ecuda::vector<double> deviceColumnSums( width ); 

// run kernel
sumColumns<<<1,width>>>( deviceMatrix, deviceColumnSums );
cudaDeviceSynchronize();

// create a vector with page-locked memory to store results off device
std::vector< double, ecuda::host_allocator<double> > hostColumnSums( width );

// copy results from device to host
deviceColumnSums >> hostColumnSums;

// ... do stuff with the results

// kernel function
__global__ void sumColumns( const ecuda::matrix<double> matrix, ecuda::vector<double> columnSums ) {

  const int threadNum = threadIdx.x;
  if( threadNum < matrix.number_columns() ) {
    double columnSum = 0;
    ecuda::matrix<double>::const_column_type column = matrix.get_column( threadNum );
    for( ecuda::matrix<double>::const_column_type::iterator iter = column.begin(); iter != column.end(); ++iter )
      columnSum += *iter;
    columnSums[threadNum] = columnSum;
  }

}
\endcode

Besides being more compact and readable, there are no naked pointers, allocation or deallocation operations, or worries about device memory padding.  STL semantics like the use of iterators in the kernel function are also much more recognizable.  With the very recent addition of C++11 support to the nvcc compiler (yah!), developers using CUDA 7 and later could even replace the column sum loop with:

\code{.cpp}
for( double x : matrix.get_column( threadNum ) ) columnSum += x;
\endcode

\section installation Installation

The library is header only, so the <tt>include</tt> subdirectory can be placed anywhere and made visible to the compiler (e.g. <tt>-I/some/path/include</tt>).  A <tt>Makefile</tt> is included that simply copies the <tt>include/ecuda</tt> directory to <tt>/usr/local/include</tt> by issuing the command:

\code
sudo make install
\endcode

The required containers can be declared in your CUDA code with, for example:

\code{.cpp}
#include <ecuda/vector.hpp>
#include <ecuda/matrix.hpp>
\endcode

If you have <a href="http://www.doxygen.org">Doxygen</a> installed, you can create a local copy of this documentation with:

\code
make docs
\endcode


\section overview Overview

\subsection overview_containers Containers

\image html containers.png "The four core containers."

The library features four containers:
  -# fixed-sized \b array
  -# variable-sized \b vector
  -# 2D \b matrix
  -# 3D \b cube

The \ref ecuda::array and \ref ecuda::vector are functionally equivalent to the identically named containers in the STL (with the array being introduced in C++11):

\code{.cpp}
ecuda::array<double,100> deviceArray( 66 ); // create fixed 100 element array filled with value 66

ecuda::vector<double> deviceVector1( 100, 66 ); // create 100 element vector filled with value 66
deviceVector1.resize( 200, 99 ); // expand the vector to 200 elements filling new elements with value 99

std::vector<int> hostVector( 50 ); // create 50 element vector in host memory
for( unsigned i = 0; i < 50; ++i ) hostVector[i] = i; // set the values to range from 0-49
ecuda::vector<int> deviceVector2( hostVector.begin(), hostVector.end() ); // create a vector in device memory and initialize with the host values
\endcode

The \ref ecuda::matrix matrix and \ref ecuda::cube cube try to implement STL conventions as faithfully as possible so using them is intuitive.

\code{.cpp}
ecuda::matrix<double> deviceMatrix( 100, 100 ); // create 100x100 matrix

std::vector<double> hostVector( 100 ); // create 100 element vector in host memory
for( unsigned i = 0; i < 100; ++i ) hostVector[i] = i; // set the values to range from 0-99

for( unsigned i = 0; i < 100; ++i ) 
   deviceMatrix[i].assign( hostVector.begin(), hostVector.end() ); // set each row of matrix to hold this sequence of values
\endcode

The containers can then be passed to kernel functions and manipulated as needed.

\code{.cpp}
__global__ 
void addColumns( const ecuda::matrix<double> srcMatrix, ecuda::array<double,100> columnSums ) {

   const int threadNum = threadIdx.x;
   typedef ecuda::matrix<double>::const_column_type ColumnType;

   if( threadNum < 100 ) {
      ColumnType column = srcMatrix.get_column(threadNum);
      for( ColumnType::iterator iter = column.begin(); iter != column.end(); ++iter )
         columnSums[threadNum] += *iter;
   }

}

ecuda::array<double,100> deviceArray;
addColumns<<<1,100>>>( deviceMatrix, deviceArray );
\endcode

\subsection overview_allocators Allocators

\image html allocators.png "The three memory allocators."

STL containers often include an optional "Allocator" template parameter so that memory allocation can be specialized.  \em ecuda uses the same design pattern to handle any allocations of device memory.  The default allocator parameter for \em ecuda containers work fine, so this aspect of the library doesn't require much mention.  However, the \ref ecuda::host_allocator allocates page-locked memory and can be useful if you want to use STL containers as a staging point to exchange data between the host and device memory (see the CUDA API documentation for the <tt>cudaMallocHost</tt> function for a more complete discussion of the advantages and considerations of using page-locked memory).

For example:

\code{.cpp}
ecuda::vector<double> deviceVector( 1000 );
// ... do stuff

std::vector< double, ecuda::host_allocator<double> > hostVector1( 1000 ); // underlying memory allocated using cudaMallocHost
std::vector<double> hostVector2( 1000 ); // underlying memory allocated using standard "new"

deviceVector >> hostVector1; // faster
deviceVector >> hostVector2; // slower
\endcode

\subsection overview_iterators Iterators

\image html iterators.png ""

Iterators are used extensively in the STL to traverse ranges of elements in a container.  They do this more optimally than, say, repeatedly calling the [] operator using an index value.  The STL classifies iterators into different categories depending on their functionality (from least to most capabilties: Input, Output < Forward < Bidirectional < Random Access).  In \em ecuda, the basic iterator is Bidirectional, with a further specialized iterator that is Random Access and requires the range being traversed to reside in contiguous memory.  A reverse iterator, which simply reverses the order of traversal is also available.  Under normal use, a developer doesn't have to worry about the iterator classes themselves.  They are typically obtained from containers using the \c begin(), \c end(), \c rbegin(), and \c rend() methods and their type defined as a container-specific <tt>typedef</tt>.  For example:

\code{.cpp}
ecuda::vector<int> deviceVector1( 1000 ); // create 1000 element vector in device memory
ecuda::vector<int>::iterator begin = deviceVector1.begin();
ecuda::vector<int> deviceVector2( begin, begin+50 ); // create a new vector initialized with the first 50 elements
\endcode

Keep in mind that operations which essentially move a contiguous block of memory to another behind the scenes using <tt>cudaMemcpy</tt> must be a \ref ecuda::contiguous_device_iterator.  Code that doesn't follow this will fail to compile.  For example:

\code{.cpp}
ecuda::matrix<int> deviceMatrix( 100, 100 );
ecuda::matrix<int>::row_type row = deviceMatrix.get_row(0);
ecuda::matrix<int>::column_type column = deviceMatrix.get_column(0);
ecuda::vector<int> deviceVector;
deviceArray.assign( row.begin(), row.end() ); // works fine
deviceArray.assign( row.rbegin(), row.rend() ); // won't compile, elements in the wrong order
deviceArray.assign( column.begin(), column.end() ); // won't compile, elements not in contiguous memory
\endcode

\subsection overview_views Views

\image html views.png ""

The core \em ecuda containers can be used without having to directly create a view, but it can be useful to know they exist in terms of the inner workings of the library.  These act as views of a subset of an existing container without carrying responsibility for the memory management.  For example, a pointer to the start of a single row or column of a \ref ecuda::matrix is provided to a \ref ecuda::contiguous_sequence_view or \ref ecuda::sequence_view, respectively, so that the memory in the matrix can be traversed and acted upon as if it were actually a sequence.  Similarly, a slice of a \ref ecuda::cube along the x-y, x-z, or y-z plane would be represented as a \ref ecuda::matrix_view or \ref ecuda::contiguous_matrix_view.  Like iterators, they are aliased through a container-specific <tt>typedef</tt> and aren't referenced explicity under normal use.  For example:

\code{.cpp}
ecuda::matrix<int> deviceMatrix( 100, 100 ); // create 100x100 matrix
ecuda::matrix<int>::row_type row = deviceMatrix.get_row(0); // is a contiguous_sequence_view
ecuda::matrix<int>::column_type column = deviceMatrix.get_row(0); // is a sequence_view

ecuda::cube<int> deviceCube( 100, 100, 100 ); // create 100x100x100 cube
ecuda::cube<int>::slice_xy_type xy = deviceCube.get_xy(0); // is a matrix_view
ecuda::cube<int>::slice_xz_type xz = deviceCube.get_xz(0); // is a contiguous_matrix_view
\endcode

\subsection overview_pointers Specialized Pointers

\warning Specialized pointers are quite far removed from a developer's interaction with the library. This section is included only for completeness.

\image html pointers.png ""

\ref ecuda::device_ptr is a reference-counting smart pointer to device memory which takes responsibility over an allocation of device memory and then automatically deallocates it when the reference count reaches zero.  It's quite similar to a std::shared_ptr.  All \em ecuda containers use a \ref ecuda::device_ptr to interface with any underlying device memory.

\ref ecuda::striding_ptr and \ref ecuda::padded_ptr are pointer-like classes (allow all of the same operations as naked pointers) but abstract away any non-contiguity in the memory being traversed.  For example, traversing the elements of a particular column of a row-major matrix is facilitated through the use of a \ref ecuda::striding_ptr where the "stride" is set to the number of columns.  CUDA also allocates 2D memory in a way that optimizes read/write operations across many threads (see \ref optimizing_threads below for more discussion on this), and results in empty padding at the end of each row.  A \ref ecuda::padded_ptr is used to traverse these elements while automatically skipping the padding.  These two specialized pointers can also be combined.  They're used internally to create efficient views and iterators.

\section optimizing_threads Optimizing Thread Operations

One of the central concerns when CUDA programming is that read/write operations to device memory are extremely time-consuming, so organizing these operations so that different threads access data in close physical proximity greatly optimizes the program.  This can be a source of optimization with any program, but it is particularly impactful for multi-threading with GPUs.  The specifics depend on the hardware, but the bus might always transfer say, 128 bits of memory per read operation.  If each data element is 8 bytes then a single read can potentially supply 16 threads with the information it requires.  When using the \ref ecuda::matrix matrix and \ref ecuda::cube containers, it's important to note how these are represented in memory and which dimension different threads should preferably access.  The rule of thumb is that the minor dimension should be the target of separate threads.  For a matrix, this is a column; for a cube, this is a depth (and secondarily, a column). For example, given these two kernel functions:

\code{.cpp}
// kernel #1
__global__ void rowSums( const ecuda::matrix<double> matrix, ecuda::vector<double> sums ) {
  const int threadNum = threadIdx.x;
  if( threadNum < matrix.number_rows() ) {
    double sum = 0.0;
    for( double x : matrix.get_row(threadNum) ) sum += x; // C++11 range-based loop (CUDA >=7.0)
    sums[threadNum] = sum;
  }
}

// kernel #2
__global__ void columnSums( const ecuda::matrix<double> matrix, ecuda::vector<double> sums ) {
  const int threadNum = threadIdx.x;
  if( threadNum < matrix.number_columns() ) {
    double sum = 0.0;
    for( double x : matrix.get_column(threadNum) ) sum += x; // C++11 range-based loop (CUDA >=7.0)
    sums[threadNum] = sum;
  }
}

// host code
ecuda::matrix<double> deviceMatrix( 1000, 1000 ); // create 1000x1000 matrix
ecuda::vector<double> deviceVector( 1000 ); // create zero-initialized vector
// ... put stuff into deviceMatrix
{
  ecuda::event start, stop;
  start.record();
  rowSums<<<1,1000>>>( deviceMatrix, deviceVector );
  stop.record();
  stop.synchronize();
  std::cout << "EXECUTION TIME: " << (stop-start) << "ms" << std::endl;
}
{
  ecuda::event start, stop;
  start.record();
  columnSums<<<1,1000>>>( deviceMatrix, deviceVector );
  stop.record();
  stop.synchronize();
  std::cout << "EXECUTION TIME: " << (stop-start) << "ms" << std::endl;
}
\endcode

The execution time of the second kernel will invariably be \em much faster than the first.

\section optional_libraries Optional Libraries

\subsection optional_library_gsl GNU Scientific Library (GSL)

There are hooks within the \ref ecuda::matrix container to copy information to and from a <tt>gsl_matrix</tt> used in the GNU Scientific Library (GSL).  Modify the <tt>HAVE_GNU_SCIENTIFIC_LIBRARY</tt> flag in the <tt>include/ecuda/config.hpp</tt> file to enable these.

\subsection optional_library_estd Extended STL (estd)

There are hooks within the \ref ecuda::matrix and \ref ecuda::cube containers to copy information to and from the equivalent \em estd containers.  This is a header-only library that contains several container types that follow the semantics of the STL much like \em ecuda itself does.  As of this writing, this library hasn't been publicly released.  Nonetheless, the methods are present awaiting the day this changes.  Modify the <tt>HAVE_ESTD_LIBRARY</tt> flag in the <tt>include/ecuda/config.hpp</tt> file to enable these.

\section performance Performance

Considerable effort was dedicated to minimizing the overhead of using \em ecuda.  Several benchmarks suggest that the overhead is minute (a constant sub-millisecond).  If you're not an experienced CUDA developer, there will likely be a performance increase if your normal approach would not have been optimized.  There are several programs in the <tt>benchmarks</tt> folder you can compile to look at the performance and the design of the test problems used.

For example, the program <tt>benchmarks/matrix_multiply</tt> performs a simple, brute-force matrix multiplication of two matrices A(5000x5000) and B(5000x5000) to get AB(5000x5000) and takes about 4.86s on an NVIDIA Tesla M2090.  Run times vary, but \em ecuda code is typically slower by <0.1ms.  The same problem solved the same way with the Intel Xeon E5-2620 CPU on the test machine takes ~18 minutes (~200X acceleration).

\section compatibility Tool Chain Compatibility

The library has been tested and compiles successfully with CUDA versions 3.2, 4.0, 4.1, 4.2, 5.0, 5.5, 6.0 in combination with GCC 4.8.1.  CUDA 6.0 and 7.0 with GCC 4.8.2 or Clang 3.5 also compiled successfully.

\em ecuda has not been tested on Windows platforms, although it should be straightforward to drop into a Visual Studio project and check. I seem to recall there were issues with using \b and and \b or (a personal C++ idiosyncracy of mine) in place of \b <tt>&&</tt> and \b <tt>||</tt> with Windows compilers in other code I've worked on.  If this is true, I will replace these in a future release.  YMMV, but I expect any problems can be easily identified and hacked into working form.

\section license License

The \em ecuda library is open source and released under the FreeBSD license.

\verbatim
Redistribution and use in source and binary forms, with or without
modification, are permitted provided that the following conditions are met:

1. Redistributions of source code must retain the above copyright notice, this
   list of conditions and the following disclaimer.
2. Redistributions in binary form must reproduce the above copyright notice,
   this list of conditions and the following disclaimer in the documentation
   and/or other materials provided with the distribution.

THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND
ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR
ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
(INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.

The views and conclusions contained in the software and documentation are those
of the authors and should not be interpreted as representing official policies,
either expressed or implied, of the FreeBSD Project.
\endverbatim

\section author Author

Scott Zuyderduyn, Ph.D.<br/>
Postdoctoral Research Fellow<br/>
Bader Lab<br/>
The University of Toronto<br/>
<br/>
Email: scott.zuyderduyn \*at\* utoronto.ca

\section acknowledgements Acknowledgements

The resources and expertise of the <a href="http://www.scinethpc.ca">SciNet</a> supercomputing centre at The University of Toronto which is home to several GPU clusters.  I used these extensively for my own scientific research (which spawned the creation of this library).

The support of the <a href="http://baderlab.org/">Bader Lab</a>, part of the <a href="http://tdccbr.med.utoronto.ca">Donnelly Centre for Cellular and Biomolecular Research</a> at The University of Toronto, where I am currently a postdoctoral fellow.

*/
