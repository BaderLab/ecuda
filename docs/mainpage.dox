/**
\mainpage

\tableofcontents

\section intro Introduction

The \em ecuda library is a set of templates fashioned after the C++ Standard Template Library (STL) that provide several useful containers for use with the CUDA API.  These containers remove most of the repetitive low-level tasks that are required when assigning, traversing, and manipulating data in device memory.  The containers can be instantiated in host code and passed to kernel functions, so familiar STL-style semantics can be used inside the device code itself.  The library is header only, so it's portable and simple to install.

The original motivation for creating the library was to remove a lot of the headaches and inherent lack of safety when using naked pointers to device memory.  For example, here's some typical looking code using the CUDA API that a) creates a matrix and b) uses the GPU to calculate the column sums:

\code{.cpp}
const size_t width = 100;
const size_t height = 100;

// create a 2D array in device memory
double* deviceMatrix;
size_t pitch;
cudaMalloc2D( &deviceMatrix, &pitch, width*sizeof(double), height );

// create linearized row-major matrix in host memory
std::vector<double> hostMatrix( width*height );

// ... do stuff to prepare matrix data

// copy matrix from host to device
cudaMemcpy2D( deviceMatrix, pitch, &hostMatrix.front(), width*sizeof(double), width*sizeof(double), height, cudaMemcpyHostToDevice );

// create an array on device to hold results
double* deviceColumnSums;
cudaMalloc( &deviceColumnSums, width*sizeof(double) );
cudaMemset( deviceColumnSums, 0, width*sizeof(double) ); // set initial values to zero

// run kernel
sumColumns<<<1,width>>>( deviceMatrix, pitch, width, height, deviceColumnSums );
cudaDeviceSynchronize();

// create an array with page-locked memory to store results off device
double* hostColumnSums;
cudaMallocHost( &hostColumnSums, width );

// copy results from device to host
cudaMemcpy( hostColumnSums, deviceColumnSums, width*sizeof(double), cudaMemcpyDeviceToHost );

//... do stuff with the results

// deallocate memory
cudaFreeHost( hostColumnSums );
cudaFree( columnSums );
cudaFree( deviceMatrix );

// kernel function
__global__ void sumColumns( const char* matrix, const size_t pitch, const size_t width, const size_t height, double* columnSums ) {

  const int threadNum = threadIdx.x;
  if( threadNum < width ) {
    double columnSum = 0;
    const char* pRow = matrix;
    for( size_t i = 0; i < height; ++i ) {
      pRow += pitch; // move pointer to next row
      const double* pElement = reinterpret_cast<const double*>(ptr); // cast to proper type!
      columnSum += pElement[threadNum];
    }
    columnSums[threadNum] = columnSum;
  }

}
\endcode

There are so many places to innocently go wrong.  Did you remember to specify <tt>width*sizeof(double)</tt> instead of just <tt>width</tt>?  Did you remember to free the allocated memory?  Did you remember to account for the padding in the matrix memory that properly aligns the memory?  Did you remember that the padding is in bytes and might not be a strict multiple of the size of the data type you're storing?  Are you an adherent to the RAII programming idiom and using CUDA makes you generally uncomfortable?

Here's the equivalent code using \em ecuda:

\code{.cpp}
const size_t width = 100;
const size_t height = 100;

// create 2D matrix on device
ecuda::matrix<double> deviceMatrix( width, height );

// create linearized row-major matrix in host memory
std::vector<double> hostMatrix( width*height );

// ... do stuff to prepare matrix data

// copy matrix from host to device
deviceMatrix.assign( hostMatrix.begin(), hostMatrix.end() );

// create vector on device to hold results
ecuda::vector<double> deviceColumnSums( width ); 

// run kernel
sumColumns<<<1,width>>>( deviceMatrix, deviceColumnSums );
cudaDeviceSynchronize();

// create a vector with page-locked memory to store results off device
std::vector< double, ecuda::host_allocator<double> > hostColumnSums( width );

// copy results from device to host
deviceColumnSums >> hostColumnSums;

// ... do stuff with the results

// kernel function
__global__ void sumColumns( const ecuda::matrix<double> matrix, ecuda::vector<double> columnSums ) {

  const int threadNum = threadIdx.x;
  if( threadNum < matrix.number_columns() ) {
    double columnSum = 0;
    ecuda::matrix<double>::const_column_type column = matrix.get_column( threadNum );
    for( ecuda::matrix<double>::const_column_type::iterator iter = column.begin(); iter != column.end(); ++iter )
      columnSum += *iter;
    columnSums[threadNum] = columnSum;
  }

}
\endcode

Besides being more compact and readable, there are no naked pointers, allocation or deallocation operations, or worries about device memory padding.  STL semantics like the use of iterators in the kernel function are also much more recognizable.  With the very recent addition of C++11 support to the nvcc compiler (yah!), developers using CUDA 7 and later could even replace the column sum loop with:

\code{.cpp}
for( double x : matrix.get_column( threadNum ) ) columnSum += x;
\endcode

\section requirements Requirements

There are no additional requirements other than your existing toolchain to compile CUDA code (minimally the CUDA API and a C++ compiler).

\section installation Installation

The library is header only, so the <tt>include</tt> subdirectory can be placed anywhere and made visible to the compiler (e.g. <tt>-I/some/path/include</tt>).  A <tt>Makefile</tt> is included that simply copies the <tt>include/ecuda</tt> directory to <tt>/usr/local/include</tt> by issuing the command:

\code
sudo make install
\endcode

The required containers can be declared in your CUDA code with, for example:

\code{.cpp}
#include <ecuda/vector.hpp>
#include <ecuda/matrix.hpp>
\endcode

If you have <a href="http://www.doxygen.org">Doxygen</a> installed, you can create a local copy of this documentation with:

\code
make docs
\endcode


\section overview Overview

\subsection overview_containers Containers

\image html containers.png "The four core containers."

The vast majority of a developer's interaction with \em ecuda is through the core containers.  Most of the rest of the API exists to support their use.  Developer's wanting to experiment with \em ecuda as fast as possible can probably do so knowing only of the core containers and applying their prior experience with STL.

The library features four containers:
  -# fixed-sized \b array
  -# variable-sized \b vector
  -# 2D \b matrix
  -# 3D \b cube

The \ref ecuda::array and \ref ecuda::vector are functionally equivalent to the identically named containers in the STL (with the array being introduced in C++11):

\code{.cpp}
ecuda::array<double,100> deviceArray; // create fixed 100 element array where elements are initially set to 0.0
deviceArray.fill( 66.0 ); // set value of all elements to 66

ecuda::vector<double> deviceVector1( 100, 66 ); // create 100 element vector filled with value 66
deviceVector1.resize( 200, 99 ); // expand the vector to 200 elements filling new elements with value 99

std::vector<int> hostVector( 50 ); // create 50 element vector in host memory
for( unsigned i = 0; i < 50; ++i ) hostVector[i] = i; // set the values to range from 0-49
ecuda::vector<int> deviceVector2( hostVector.begin(), hostVector.end() ); // create a vector in device memory and initialize with the host values
\endcode

The \ref ecuda::matrix matrix and \ref ecuda::cube cube try to implement STL conventions as faithfully as possible so using them is intuitive.

\code{.cpp}
ecuda::matrix<double> deviceMatrix( 100, 100 ); // create 100x100 matrix

std::vector<double> hostVector( 100 ); // create 100 element vector in host memory
for( unsigned i = 0; i < 100; ++i ) hostVector[i] = i; // set the values to range from 0-99

for( unsigned i = 0; i < 100; ++i ) 
   deviceMatrix[i].assign( hostVector.begin(), hostVector.end() ); // set each row of matrix to hold this sequence of values
\endcode

The containers can then be passed to kernel functions and manipulated as needed.

\code{.cpp}
__global__ 
void addColumns( const ecuda::matrix<double> srcMatrix, ecuda::array<double,100> columnSums ) {

   const int threadNum = threadIdx.x;
   typedef ecuda::matrix<double>::const_column_type ColumnType;

   if( threadNum < 100 ) {
      ColumnType column = srcMatrix.get_column(threadNum);
      for( ColumnType::iterator iter = column.begin(); iter != column.end(); ++iter )
         columnSums[threadNum] += *iter;
   }

}

ecuda::array<double,100> deviceArray;
addColumns<<<1,100>>>( deviceMatrix, deviceArray );
\endcode

One minor, but vital, departure from the usual STL semantics is transferring the contents of device-memory bound containers to host containers.  For example, the following won't work:

\code{.cpp}
ecuda::vector<int> deviceVector( 100 );
std::vector<int> hostVector( deviceVector.begin(), deviceVector.end() ); // won't compile
\endcode

Since STL containers can't access device memory directly, responsibility for device to host transfer resides with the \em ecuda containers.  This is universally available from the \em ecuda containers and derivatives via the >> operator.

\code{.cpp}
// create device vector
ecuda::vector<int> deviceVector( 100 );

std::vector<int> hostVector( deviceVector.size() );
deviceVector >> hostVector; // copy from device to host container

// create device matrix
ecuda::matrix<double> deviceMatrix( 200, 500 );

std::vector<int> hostMatrix( deviceMatrix.size() );
deviceMatrix >> hostMatrix; // copy from device to host container

std::vector<int> hostRow( deviceMatrix.number_columns() );
deviceMatrix[99] >> hostRow; // copy row #99 to host container

// ... etc.
\endcode

\subsection overview_allocators Allocators

\image html allocators.png "The three memory allocators."

STL containers often include an optional "Allocator" template parameter so that memory allocation can be specialized.  \em ecuda uses the same design pattern to handle any allocations of device memory.  The default allocator parameter for \em ecuda containers work fine, so this aspect of the library doesn't require much mention.  However, the \ref ecuda::host_allocator allocates page-locked memory and can be useful if you want to use STL containers as a staging point to exchange data between the host and device memory (see the CUDA API documentation for the <tt>cudaMallocHost</tt> function for a more complete discussion of the advantages and considerations of using page-locked memory).

For example:

\code{.cpp}
ecuda::vector<double> deviceVector( 1000 );
// ... do stuff

std::vector< double, ecuda::host_allocator<double> > hostVector1( 1000 ); // underlying memory allocated using cudaMallocHost
std::vector<double> hostVector2( 1000 ); // underlying memory allocated using standard "new"

deviceVector >> hostVector1; // faster
deviceVector >> hostVector2; // slower
\endcode

\subsection overview_iterators Iterators

\image html iterators.png ""

Iterators are used extensively in the STL to traverse ranges of elements in a container.  They do this more optimally than, say, repeatedly calling the [] operator using an index value.  The STL classifies iterators into different categories depending on their functionality (from least to most capabilties: Input, Output < Forward < Bidirectional < Random Access).  In \em ecuda there are two additional types defined: a Contiguous Device and Noncontiguous Device iterator.  A general-purpose reverse device iterator is also defined.  Under normal use, a developer doesn't have to worry about the iterator classes themselves.  They are typically obtained from containers using the \c begin(), \c end(), \c rbegin(), and \c rend() methods and their type defined as a container-specific <tt>typedef</tt>.  For example:

\code{.cpp}
ecuda::vector<int> deviceVector1( 1000 ); // create 1000 element vector in device memory
ecuda::vector<int>::iterator begin = deviceVector1.begin();
ecuda::vector<int> deviceVector2( begin, begin+50 ); // create a new device vector initialized with the first 50 elements
\endcode

The category of iterator is checked at compile-time to determine whether (and how) an iterator-based operation can be carried out.  This is especially important for operations performed in host code where copying memory from host to device, device to host, and device to device require both the source and destination to be contiguous blocks of memory.

See the next section on \ref overview_models "Memory Models" for information on how iterators are used and the practical differences between different device iterators.

\subsection overview_models Memory Models

All core containers (array, vector, matrix, cube) are derived from one of two model classes: \ref ecuda::__device_sequence or \ref ecuda::__device_grid (which is itself derived from \ref ecuda::__device_sequence).  Specialized views of the data in these containers are also derived from these models.  C++ metaprogramming techniques are used to determine at compile-time what capabilities a model should have, and how things such as memory transfers should be performed.

The models are not referenced directly (although a developer is free to make their own custom containers based on these model classes).  In practice, they are referenced through a container-specific <tt>typedef</tt>.  For example:

\code{.cpp}
ecuda::matrix<int> deviceMatrix( 100, 100 ); // create 100x100 matrix which is a specialized __device_grid
ecuda::matrix<int>::row_type row = deviceMatrix.get_row(0); // is a __device_sequence with capabilities of contiguous memory
ecuda::matrix<int>::column_type column = deviceMatrix.get_column(0); // is a __device_sequence with capabilities of non-contiguous memory

ecuda::cube<int> deviceCube( 100, 100, 100 ); // create 100x100x100 cube which is a specialized __device_grid
ecuda::cube<int>::slice_xy_type xy = deviceCube.get_xy(0); // is a __device_grid with non-contiguous rows and non-contiguous columns
ecuda::cube<int>::slice_xz_type xz = deviceCube.get_xz(0); // is a __device_grid with non-contiugous rows and contiguous columns
\endcode

Since the capabilities of a given model are interpreted at compile-time, they can perform a given task without the overhead of checking the properties of containers or data at run-time.  Compile-time considerations are primarily of two kinds: 1) is the source and/or destination of the operation a block of contiguous memory? and 2) is this operation occuring on the host or the device?

\code{.cpp}
ecuda::matrix<int> deviceMatrix( 100, 100 );
ecuda::matrix<int>::row_type row = deviceMatrix.get_row(0);
ecuda::matrix<int>::column_type column = deviceMatrix.get_column(0);

ecuda::vector<int> deviceVector( 100 );
std::vector<int> hostVector( 100 ); // STL vectors are represented in contiguous memory

row >> deviceVector; // direct transfer of data from device to host via cudaMemcpy
column >> deviceVector; // throws an exception because column data is not contiguous
row >> hostVector; // direct transfer of data from device to host via cudaMemcpy

row.assign( deviceVector.begin(), deviceVector.end() ); // direct transfer of data from device to device via cudaMemcpy
row.assign( hostVector.begin(), hostVector.end() ); // direct transfer of data from host to device via cudaMemcpy
row.assign( column.begin(), column.end() ); // throws an exception when called from host because column data is on the device and is not contiguous

std::list<int> hostList( 100 ); // STL lists are represented in non-contiguous memory
row.assign( hostList.begin(), hostList.end() ); // list is non-contiguous so API copies it to a contiguous staging area before transfering to device

// kernel function can't use assign
__global__ void transferColumnToRow( ecuda::matrix<int>::row_type row, ecuda::matrix<int>::column_type column ) {
  row.assign( column.begin(), column.end() ); // won't compile, can't enforce assign() requirements in device code
  column.assign( row.begin(), row.end() ); // won't compile, can't enforce assign() requirements in device code
}

// kernel function working alternative
__global__ void transferColumnToRow( ecuda::matrix<int>::row_type row, ecuda::matrix<int>::column_type column ) {
  ecuda::matrix<int>::row_type::iterator rowIter = row.begin();
  ecuda::matrix<int>::column_type::iterator columnIter = column.begin();
  // we take responsibility for transfering column to row on the device
  for( ; rowIter != row.end() && columnIter != column.end(); ++rowIter, ++columnIter ) *rowIter = *columnIter;
}

\endcode

\subsection overview_pointers Specialized Pointers

\warning Specialized pointers are quite far removed from a developer's interaction with the library. This section is included only for completeness.

\image html pointers.png ""

\ref ecuda::device_ptr is a reference-counting smart pointer to device memory which takes responsibility over an allocation of device memory and then automatically deallocates it when all objects depending on it go out of scope.  It's quite similar to a std::shared_ptr.  All \em ecuda containers use a \ref ecuda::device_ptr to interface with any underlying device memory.

\ref ecuda::striding_ptr and \ref ecuda::padded_ptr are pointer-like classes (allow all of the same operations as naked pointers) but abstract away any non-contiguity in the memory being traversed.  For example, traversing the elements of a particular column of a row-major matrix is facilitated through the use of a \ref ecuda::striding_ptr where the "stride" is set to the number of columns.  CUDA also allocates 2D memory in a way that optimizes read/write operations across many threads (see \ref optimizing_threads below for more discussion on this), and results in empty padding at the end of each row.  A \ref ecuda::padded_ptr is used to traverse these elements while automatically skipping the padding.  These two specialized pointers can also be combined.  They're used internally to create efficient iterators and views of a particular subset of container data.

\subsection overview_misc Miscellaneous

The library contains additional classes, functions, and macros to simplify some common CUDA programming tasks.

\subsubsection overview_misc_cudawrapper Capturing CUDA API Errors

A macro called CUDA_CALL is defined that captures errors from any of the low-level CUDA API functions and throws an \ref ecuda::cuda_error exception.

\code{.cpp}
try {
  CUDA_CALL( cudaDeviceSynchronize() );
} catch( ecuda::cuda_error& ex ) {
  std::cerr << "CUDA API error: " << ex.what() << std::endl;
}
\endcode

The macro is also used with any such calls within the \em ecuda library.

Additionally, a macro called CUDA_CHECK_ERRORS can be used after starting a kernel to capture any error that would normally be checked using cudaGetLastError(). Any error will be converted to an \ref ecuda::cuda_error exception and thrown.

\code{.cpp}
try {
  myKernel<<<10,1000>>>( ... );
  CUDA_CHECK_ERRORS();
} catch( ecuda::cuda_error& ex ) {
  std::cerr << "CUDA kernel error: " << ex.what() << std::endl;
}
\endcode

\subsubsection overview_misc_cstyle Using C-style Arrays

The \ref ecuda::host_array_proxy is a simple wrapper that allows C-style arrays to function as a standard container.  This is useful for incorporating other libraries, like the GNU Scientific Library, that still use naked pointers.

\code{.cpp}
gsl_matrix* mat = gsl_matrix_alloc( 10, 20 );
// ... prepare matrix values
ecuda::host_array_proxy<double> proxy( mat->data, 10*20 );
ecuda::matrix<double> deviceMatrix( 10, 20 );
deviceMatrix.assign( proxy.begin(), proxy.end() ); // copies gsl_matrix to device matrix via the proxy
deviceMatrix >> proxy; // copies device matrix to gsl_matrix via the proxy
\endcode

\subsubsection overview_misc_events CUDA Events

A wrapper around CUDA event objects called \ref ecuda::event makes these more C++-like.

\code{.cpp}
// record kernel execution time
ecuda::event start, stop;

start.record(); // record start time
myKernel<<<10,1000>>>( ... );
stop.record(); // record stop time

stop.synchronize(); // kernel execution is asynchronous, wait until it finishes

std::cerr << "EXECUTION TIME: " << ( stop-start ) << "milliseconds" << std::endl;
\endcode

\section optimizing_threads Optimizing Thread Operations

One of the central concerns when CUDA programming is that read/write operations to device memory are extremely time-consuming, so organizing these operations so that different threads access data in close physical proximity greatly optimizes the program.  This can be a source of optimization with any program, but it is particularly impactful for multi-threading with GPUs.  The specifics depend on the hardware, but the bus might always transfer say, 128 bits of memory per read operation.  If each data element is 8 bytes then a single read can potentially supply 16 threads with the information it requires.  When using the \ref ecuda::matrix matrix and \ref ecuda::cube containers, it's important to note how these are represented in memory and which dimension different threads should preferably access.  The rule of thumb is that the minor dimension should be the target of separate threads.  For a matrix, this is a column; for a cube, this is a depth (and secondarily, a column). For example, given these two kernel functions:

\code{.cpp}
// kernel #1
__global__ void rowSums( const ecuda::matrix<double> matrix, ecuda::vector<double> sums ) {
  const int threadNum = threadIdx.x;
  if( threadNum < matrix.number_rows() ) {
    double sum = 0.0;
    for( double x : matrix.get_row(threadNum) ) sum += x; // C++11 range-based loop (CUDA >=7.0)
    sums[threadNum] = sum;
  }
}

// kernel #2
__global__ void columnSums( const ecuda::matrix<double> matrix, ecuda::vector<double> sums ) {
  const int threadNum = threadIdx.x;
  if( threadNum < matrix.number_columns() ) {
    double sum = 0.0;
    for( double x : matrix.get_column(threadNum) ) sum += x; // C++11 range-based loop (CUDA >=7.0)
    sums[threadNum] = sum;
  }
}

// host code
ecuda::matrix<double> deviceMatrix( 1000, 1000 ); // create 1000x1000 matrix
ecuda::vector<double> deviceVector( 1000 ); // create zero-initialized vector
// ... put stuff into deviceMatrix
{
  ecuda::event start, stop;
  start.record();
  rowSums<<<1,1000>>>( deviceMatrix, deviceVector );
  stop.record();
  stop.synchronize();
  std::cout << "EXECUTION TIME: " << (stop-start) << "ms" << std::endl;
}
{
  ecuda::event start, stop;
  start.record();
  columnSums<<<1,1000>>>( deviceMatrix, deviceVector );
  stop.record();
  stop.synchronize();
  std::cout << "EXECUTION TIME: " << (stop-start) << "ms" << std::endl;
}
\endcode

The execution time of the second kernel will invariably be \em much faster than the first.

\section performance Performance

Considerable effort was dedicated to minimizing the overhead of using \em ecuda.  Several benchmarks suggest that the overhead is minute (a constant sub-millisecond).  If you're not an experienced CUDA developer, there will likely be a performance increase if your normal approach would not have been optimized.  There are several programs in the <tt>benchmarks</tt> folder you can compile to look at the performance and the design of the test problems used.

For example, the program <tt>benchmarks/matrix_multiply</tt> performs a simple, brute-force matrix multiplication of two matrices A(5000x5000) and B(5000x5000) to get AB(5000x5000) and takes about 4.86s on an NVIDIA Tesla M2090.  Run times vary, but \em ecuda code is typically slower by <0.1ms.  The same problem solved the same way with the Intel Xeon E5-2620 CPU on the test machine takes ~18 minutes (~200X acceleration).

\section compatibility Tool Chain Compatibility

The library has been tested and compiles successfully with CUDA versions 3.2, 4.0, 4.1, 4.2, 5.0, 5.5, 6.0 in combination with GCC 4.8.1.  CUDA 6.0 and 7.0 with GCC 4.8.2 or Clang 3.5 also compiled successfully.

\em ecuda has not been tested on Windows platforms, although it should be straightforward to drop into a Visual Studio project and check. I seem to recall there were issues with using \b and and \b or (a personal C++ idiosyncracy of mine) in place of \b <tt>&&</tt> and \b <tt>||</tt> with Windows compilers in other code I've worked on.  If this is true, I will replace these in a future release.  YMMV, but I expect any problems can be easily identified and hacked into working form.

\section license License

The \em ecuda library is open source and released under the FreeBSD license.

\verbatim
Redistribution and use in source and binary forms, with or without
modification, are permitted provided that the following conditions are met:

1. Redistributions of source code must retain the above copyright notice, this
   list of conditions and the following disclaimer.
2. Redistributions in binary form must reproduce the above copyright notice,
   this list of conditions and the following disclaimer in the documentation
   and/or other materials provided with the distribution.

THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND
ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR
ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
(INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.

The views and conclusions contained in the software and documentation are those
of the authors and should not be interpreted as representing official policies,
either expressed or implied, of the FreeBSD Project.
\endverbatim

\section author Author

Scott Zuyderduyn, Ph.D.<br/>
Postdoctoral Research Fellow<br/>
Bader Lab<br/>
The University of Toronto<br/>
<br/>
Email: scott.zuyderduyn \*at\* utoronto.ca

\section acknowledgements Acknowledgements

The resources and expertise of the <a href="http://www.scinethpc.ca">SciNet</a> supercomputing centre at The University of Toronto which is home to several GPU clusters.  I used these extensively for my own scientific research (which spawned the creation of this library).

The support of the <a href="http://baderlab.org/">Bader Lab</a>, part of the <a href="http://tdccbr.med.utoronto.ca">Donnelly Centre for Cellular and Biomolecular Research</a> at The University of Toronto, where I am currently a postdoctoral fellow.

*/
